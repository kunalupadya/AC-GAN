{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "nz = 100\n",
    "num_classes = 10\n",
    "d_lr = 0.0002\n",
    "g_lr = 0.0002\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "class netG(nn.Module):\n",
    "    def __init__(self, lenz):\n",
    "        super(netG, self).__init__()\n",
    "        self.lenz = lenz\n",
    "        self.l = nn.Linear(110,512)\n",
    "\n",
    "        self.t1 = nn.ConvTranspose2d(512, 384, 4, (2,2),1)\n",
    "        self.bn1 = nn.BatchNorm2d(384)\n",
    "        self.t2 = nn.ConvTranspose2d(384, 192, 4, (2, 2),1)\n",
    "        self.bn2 = nn.BatchNorm2d(192)\n",
    "        self.t3 = nn.ConvTranspose2d(192, 148, 4, (2, 2),1)\n",
    "        self.bn3 = nn.BatchNorm2d(148)\n",
    "        self.t4 = nn.ConvTranspose2d(148, 92, 4, (2, 2),1)\n",
    "        \n",
    "        self.bn4 = nn.BatchNorm2d(92)\n",
    "        self.t5 = nn.ConvTranspose2d(92, 3, 4, (2, 2),1)\n",
    "        \n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "#         print(x.shape)\n",
    "        x = x.view(-1, self.lenz)\n",
    "#         print(x.shape)\n",
    "        x = self.l(x).view(-1, 512, 1, 1)\n",
    "#         print(x.shape)\n",
    "        x = F.relu(self.bn1(self.t1(x)))\n",
    "#         print(x.shape)\n",
    "        x = F.relu(self.bn2(self.t2(x)))\n",
    "#         print(x.shape)\n",
    "        x = F.relu(self.bn3(self.t3(x)))\n",
    "        x = F.relu(self.bn4(self.t4(x)))\n",
    "#         print(x.shape)\n",
    "        x = F.tanh(self.t5(x))\n",
    "#         print(x.shape)\n",
    "        return x\n",
    "\n",
    "\n",
    "class netD(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(netD, self).__init__()\n",
    "        self.c1 = nn.Conv2d(3, 16, (3, 3), (2, 2), 1)\n",
    "        self.c2 = nn.Conv2d(16, 32, (3, 3), (1, 1), 1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.c3 = nn.Conv2d(32, 64, (3, 3), (2, 2), 1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.c4 = nn.Conv2d(64, 128, (3, 3), (1, 1), 1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.c5 = nn.Conv2d(128, 256, (3, 3), (2, 2), 1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.c6 = nn.Conv2d(256, 512, (3, 3), (1, 1), 1)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        self.d1 = nn.Linear(4 * 4 * 512, 1)\n",
    "        self.d2 = nn.Linear(4 * 4 * 512, num_classes)\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "#         print(x.shape)\n",
    "        x = F.dropout(F.leaky_relu(self.c1(x)),0.5)\n",
    "        x = F.dropout(self.bn1(F.leaky_relu(self.c2(x))),0.5)\n",
    "        x = F.dropout(self.bn2(F.leaky_relu(self.c3(x))),0.5)\n",
    "        x = F.dropout(self.bn3(F.leaky_relu(self.c4(x))),0.5)\n",
    "        x = F.dropout(self.bn4(F.leaky_relu(self.c5(x))),0.5)\n",
    "        x = F.dropout(self.bn5(F.leaky_relu(self.c6(x))),0.5)\n",
    "        x = x.view(-1, 4 * 4 * 512)\n",
    "        real_or_fake = self.d1(x)\n",
    "        classes = self.d2(x)\n",
    "        return real_or_fake, classes\n",
    "    \n",
    "def sample_image(n_row, batches_done, generator):\n",
    "    \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\"\n",
    "    # Sample noise\n",
    "    batch_size = n_row ** 2\n",
    "    noise = np.random.normal(0, 1, (batch_size, nz))\n",
    "    np_gen_label = np.random.randint(0, num_classes, batch_size)\n",
    "    onehot = np.zeros((batch_size, num_classes))\n",
    "    onehot[np.arange(batch_size), np_gen_label] = 1\n",
    "    z = np.concatenate((noise, onehot), axis=1)\n",
    "    z = torch.from_numpy(z).float().to(device)\n",
    "    \n",
    "    \n",
    "    gen_imgs = generator(z)\n",
    "    save_image(gen_imgs.data, \"images/%d.png\" % batches_done, nrow=n_row, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Epoch: 0\n",
      "G Loss: 4.570998\n",
      "D Loss 2.799372\n",
      "\n",
      "\n",
      "Epoch: 1\n",
      "G Loss: 5.424906\n",
      "D Loss 2.778892\n",
      "\n",
      "\n",
      "Epoch: 2\n",
      "G Loss: 4.308732\n",
      "D Loss 2.996513\n",
      "\n",
      "\n",
      "Epoch: 3\n",
      "G Loss: 4.801156\n",
      "D Loss 2.464944\n",
      "\n",
      "\n",
      "Epoch: 4\n",
      "G Loss: 4.644513\n",
      "D Loss 2.767070\n",
      "\n",
      "\n",
      "Epoch: 5\n",
      "G Loss: 3.502532\n",
      "D Loss 2.358132\n",
      "\n",
      "\n",
      "Epoch: 6\n",
      "G Loss: 2.351707\n",
      "D Loss 2.086272\n",
      "\n",
      "\n",
      "Epoch: 7\n",
      "G Loss: 2.090488\n",
      "D Loss 1.547239\n",
      "\n",
      "\n",
      "Epoch: 8\n",
      "G Loss: 1.949486\n",
      "D Loss 1.616706\n",
      "\n",
      "\n",
      "Epoch: 9\n",
      "G Loss: 1.884300\n",
      "D Loss 1.553009\n",
      "\n",
      "\n",
      "Epoch: 10\n",
      "G Loss: 2.260978\n",
      "D Loss 1.547080\n",
      "\n",
      "\n",
      "Epoch: 11\n",
      "G Loss: 2.076491\n",
      "D Loss 1.527310\n",
      "\n",
      "\n",
      "Epoch: 12\n",
      "G Loss: 2.087011\n",
      "D Loss 1.878270\n",
      "\n",
      "\n",
      "Epoch: 13\n",
      "G Loss: 1.979293\n",
      "D Loss 1.626159\n",
      "\n",
      "\n",
      "Epoch: 14\n",
      "G Loss: 1.745069\n",
      "D Loss 1.405434\n",
      "\n",
      "\n",
      "Epoch: 15\n",
      "G Loss: 1.863899\n",
      "D Loss 1.457474\n",
      "\n",
      "\n",
      "Epoch: 16\n",
      "G Loss: 2.281406\n",
      "D Loss 1.556971\n",
      "\n",
      "\n",
      "Epoch: 17\n",
      "G Loss: 1.892984\n",
      "D Loss 1.374398\n",
      "\n",
      "\n",
      "Epoch: 18\n",
      "G Loss: 1.701557\n",
      "D Loss 1.390654\n",
      "\n",
      "\n",
      "Epoch: 19\n",
      "G Loss: 1.565010\n",
      "D Loss 1.431092\n",
      "\n",
      "\n",
      "Epoch: 20\n",
      "G Loss: 2.897371\n",
      "D Loss 1.372189\n",
      "\n",
      "\n",
      "Epoch: 21\n",
      "G Loss: 1.693636\n",
      "D Loss 1.317952\n",
      "\n",
      "\n",
      "Epoch: 22\n",
      "G Loss: 2.081895\n",
      "D Loss 1.340992\n",
      "\n",
      "\n",
      "Epoch: 23\n",
      "G Loss: 1.764317\n",
      "D Loss 1.316609\n",
      "\n",
      "\n",
      "Epoch: 24\n",
      "G Loss: 1.666074\n",
      "D Loss 1.530065\n",
      "\n",
      "\n",
      "Epoch: 25\n",
      "G Loss: 1.907091\n",
      "D Loss 1.417084\n",
      "\n",
      "\n",
      "Epoch: 26\n",
      "G Loss: 1.779455\n",
      "D Loss 1.367875\n",
      "\n",
      "\n",
      "Epoch: 27\n",
      "G Loss: 1.987582\n",
      "D Loss 1.398597\n",
      "\n",
      "\n",
      "Epoch: 28\n",
      "G Loss: 1.941183\n",
      "D Loss 1.355896\n",
      "\n",
      "\n",
      "Epoch: 29\n",
      "G Loss: 1.998249\n",
      "D Loss 1.438853\n",
      "\n",
      "\n",
      "Epoch: 30\n",
      "G Loss: 3.044050\n",
      "D Loss 1.328612\n",
      "\n",
      "\n",
      "Epoch: 31\n",
      "G Loss: 1.403734\n",
      "D Loss 1.507153\n",
      "\n",
      "\n",
      "Epoch: 32\n",
      "G Loss: 1.944145\n",
      "D Loss 1.385612\n",
      "\n",
      "\n",
      "Epoch: 33\n",
      "G Loss: 1.601531\n",
      "D Loss 1.467589\n",
      "\n",
      "\n",
      "Epoch: 34\n",
      "G Loss: 1.588573\n",
      "D Loss 1.389754\n",
      "\n",
      "\n",
      "Epoch: 35\n",
      "G Loss: 1.545041\n",
      "D Loss 1.241291\n",
      "\n",
      "\n",
      "Epoch: 36\n",
      "G Loss: 1.931104\n",
      "D Loss 1.350113\n",
      "\n",
      "\n",
      "Epoch: 37\n",
      "G Loss: 2.191620\n",
      "D Loss 1.385713\n",
      "\n",
      "\n",
      "Epoch: 38\n",
      "G Loss: 1.795426\n",
      "D Loss 1.233168\n",
      "\n",
      "\n",
      "Epoch: 39\n",
      "G Loss: 1.308015\n",
      "D Loss 1.333390\n",
      "\n",
      "\n",
      "Epoch: 40\n",
      "G Loss: 2.294010\n",
      "D Loss 1.383631\n",
      "\n",
      "\n",
      "Epoch: 41\n",
      "G Loss: 2.067516\n",
      "D Loss 1.218148\n",
      "\n",
      "\n",
      "Epoch: 42\n",
      "G Loss: 2.188835\n",
      "D Loss 1.276084\n",
      "\n",
      "\n",
      "Epoch: 43\n",
      "G Loss: 1.860345\n",
      "D Loss 1.214046\n",
      "\n",
      "\n",
      "Epoch: 44\n",
      "G Loss: 1.611908\n",
      "D Loss 1.127362\n",
      "\n",
      "\n",
      "Epoch: 45\n",
      "G Loss: 2.171010\n",
      "D Loss 1.301488\n",
      "\n",
      "\n",
      "Epoch: 46\n",
      "G Loss: 1.828653\n",
      "D Loss 1.288676\n",
      "\n",
      "\n",
      "Epoch: 47\n",
      "G Loss: 2.119457\n",
      "D Loss 1.277390\n",
      "\n",
      "\n",
      "Epoch: 48\n",
      "G Loss: 1.624389\n",
      "D Loss 1.217983\n",
      "\n",
      "\n",
      "Epoch: 49\n",
      "G Loss: 1.317514\n",
      "D Loss 1.425346\n",
      "\n",
      "\n",
      "Epoch: 50\n",
      "G Loss: 1.792878\n",
      "D Loss 1.235173\n",
      "\n",
      "\n",
      "Epoch: 51\n",
      "G Loss: 2.689611\n",
      "D Loss 1.160959\n",
      "\n",
      "\n",
      "Epoch: 52\n",
      "G Loss: 2.078348\n",
      "D Loss 1.159163\n",
      "\n",
      "\n",
      "Epoch: 53\n",
      "G Loss: 2.632906\n",
      "D Loss 1.245403\n",
      "\n",
      "\n",
      "Epoch: 54\n",
      "G Loss: 1.858648\n",
      "D Loss 1.143482\n",
      "\n",
      "\n",
      "Epoch: 55\n",
      "G Loss: 2.171755\n",
      "D Loss 1.146533\n",
      "\n",
      "\n",
      "Epoch: 56\n",
      "G Loss: 3.882966\n",
      "D Loss 1.566734\n",
      "\n",
      "\n",
      "Epoch: 57\n",
      "G Loss: 2.598147\n",
      "D Loss 1.167883\n",
      "\n",
      "\n",
      "Epoch: 58\n",
      "G Loss: 2.447953\n",
      "D Loss 1.111823\n",
      "\n",
      "\n",
      "Epoch: 59\n",
      "G Loss: 2.268906\n",
      "D Loss 1.057871\n",
      "\n",
      "\n",
      "Epoch: 60\n",
      "G Loss: 2.816149\n",
      "D Loss 1.380483\n",
      "\n",
      "\n",
      "Epoch: 61\n",
      "G Loss: 2.631908\n",
      "D Loss 1.113695\n",
      "\n",
      "\n",
      "Epoch: 62\n",
      "G Loss: 2.103613\n",
      "D Loss 1.194534\n",
      "\n",
      "\n",
      "Epoch: 63\n",
      "G Loss: 2.379589\n",
      "D Loss 1.096476\n",
      "\n",
      "\n",
      "Epoch: 64\n",
      "G Loss: 3.125010\n",
      "D Loss 1.190289\n",
      "\n",
      "\n",
      "Epoch: 65\n",
      "G Loss: 1.755164\n",
      "D Loss 1.292295\n",
      "\n",
      "\n",
      "Epoch: 66\n",
      "G Loss: 2.616740\n",
      "D Loss 1.265286\n",
      "\n",
      "\n",
      "Epoch: 67\n",
      "G Loss: 3.378321\n",
      "D Loss 1.077791\n",
      "\n",
      "\n",
      "Epoch: 68\n",
      "G Loss: 3.229456\n",
      "D Loss 1.038106\n",
      "\n",
      "\n",
      "Epoch: 69\n",
      "G Loss: 1.852492\n",
      "D Loss 1.054668\n",
      "\n",
      "\n",
      "Epoch: 70\n",
      "G Loss: 3.618274\n",
      "D Loss 0.985243\n",
      "\n",
      "\n",
      "Epoch: 71\n",
      "G Loss: 2.086313\n",
      "D Loss 1.344410\n",
      "\n",
      "\n",
      "Epoch: 72\n",
      "G Loss: 3.359328\n",
      "D Loss 0.951964\n",
      "\n",
      "\n",
      "Epoch: 73\n",
      "G Loss: 2.790225\n",
      "D Loss 0.888179\n",
      "\n",
      "\n",
      "Epoch: 74\n",
      "G Loss: 3.049771\n",
      "D Loss 0.951622\n",
      "\n",
      "\n",
      "Epoch: 75\n",
      "G Loss: 1.845462\n",
      "D Loss 1.295815\n",
      "\n",
      "\n",
      "Epoch: 76\n",
      "G Loss: 2.340410\n",
      "D Loss 1.109514\n",
      "\n",
      "\n",
      "Epoch: 77\n",
      "G Loss: 2.122605\n",
      "D Loss 1.142807\n",
      "\n",
      "\n",
      "Epoch: 78\n",
      "G Loss: 3.024935\n",
      "D Loss 1.002181\n",
      "\n",
      "\n",
      "Epoch: 79\n",
      "G Loss: 3.191182\n",
      "D Loss 1.095143\n",
      "\n",
      "\n",
      "Epoch: 80\n",
      "G Loss: 2.393817\n",
      "D Loss 1.131723\n",
      "\n",
      "\n",
      "Epoch: 81\n",
      "G Loss: 2.599380\n",
      "D Loss 0.931852\n",
      "\n",
      "\n",
      "Epoch: 82\n",
      "G Loss: 2.542874\n",
      "D Loss 0.993272\n",
      "\n",
      "\n",
      "Epoch: 83\n",
      "G Loss: 4.635223\n",
      "D Loss 1.095620\n",
      "\n",
      "\n",
      "Epoch: 84\n",
      "G Loss: 3.363852\n",
      "D Loss 0.932248\n",
      "\n",
      "\n",
      "Epoch: 85\n",
      "G Loss: 5.177273\n",
      "D Loss 0.877408\n",
      "\n",
      "\n",
      "Epoch: 86\n",
      "G Loss: 3.346795\n",
      "D Loss 0.913410\n",
      "\n",
      "\n",
      "Epoch: 87\n",
      "G Loss: 2.784819\n",
      "D Loss 1.280835\n",
      "\n",
      "\n",
      "Epoch: 88\n",
      "G Loss: 2.496683\n",
      "D Loss 0.977284\n",
      "\n",
      "\n",
      "Epoch: 89\n",
      "G Loss: 3.060658\n",
      "D Loss 1.153621\n",
      "\n",
      "\n",
      "Epoch: 90\n",
      "G Loss: 3.933634\n",
      "D Loss 1.213712\n",
      "\n",
      "\n",
      "Epoch: 91\n",
      "G Loss: 3.195194\n",
      "D Loss 0.877563\n",
      "\n",
      "\n",
      "Epoch: 92\n",
      "G Loss: 2.528463\n",
      "D Loss 0.885294\n",
      "\n",
      "\n",
      "Epoch: 93\n",
      "G Loss: 3.071203\n",
      "D Loss 0.986483\n",
      "\n",
      "\n",
      "Epoch: 94\n",
      "G Loss: 5.003314\n",
      "D Loss 1.406687\n",
      "\n",
      "\n",
      "Epoch: 95\n",
      "G Loss: 2.097971\n",
      "D Loss 0.971724\n",
      "\n",
      "\n",
      "Epoch: 96\n",
      "G Loss: 3.517010\n",
      "D Loss 1.081107\n",
      "\n",
      "\n",
      "Epoch: 97\n",
      "G Loss: 1.730685\n",
      "D Loss 0.972022\n",
      "\n",
      "\n",
      "Epoch: 98\n",
      "G Loss: 2.607451\n",
      "D Loss 0.865951\n",
      "\n",
      "\n",
      "Epoch: 99\n",
      "G Loss: 3.122317\n",
      "D Loss 0.846667\n",
      "\n",
      "\n",
      "Epoch: 100\n",
      "G Loss: 1.838679\n",
      "D Loss 1.026634\n",
      "\n",
      "\n",
      "Epoch: 101\n",
      "G Loss: 3.751496\n",
      "D Loss 0.846140\n",
      "\n",
      "\n",
      "Epoch: 102\n",
      "G Loss: 3.166909\n",
      "D Loss 0.996338\n",
      "\n",
      "\n",
      "Epoch: 103\n",
      "G Loss: 2.779354\n",
      "D Loss 0.967305\n",
      "\n",
      "\n",
      "Epoch: 104\n",
      "G Loss: 2.682271\n",
      "D Loss 0.936604\n",
      "\n",
      "\n",
      "Epoch: 105\n",
      "G Loss: 3.400703\n",
      "D Loss 0.868457\n",
      "\n",
      "\n",
      "Epoch: 106\n",
      "G Loss: 1.489049\n",
      "D Loss 1.083718\n",
      "\n",
      "\n",
      "Epoch: 107\n",
      "G Loss: 5.949809\n",
      "D Loss 1.069726\n",
      "\n",
      "\n",
      "Epoch: 108\n",
      "G Loss: 1.773748\n",
      "D Loss 0.999696\n",
      "\n",
      "\n",
      "Epoch: 109\n",
      "G Loss: 2.835011\n",
      "D Loss 0.829840\n",
      "\n",
      "\n",
      "Epoch: 110\n",
      "G Loss: 5.662780\n",
      "D Loss 0.864083\n",
      "\n",
      "\n",
      "Epoch: 111\n",
      "G Loss: 4.434917\n",
      "D Loss 0.997296\n",
      "\n",
      "\n",
      "Epoch: 112\n",
      "G Loss: 3.153396\n",
      "D Loss 0.785528\n",
      "\n",
      "\n",
      "Epoch: 113\n",
      "G Loss: 3.652623\n",
      "D Loss 0.852006\n",
      "\n",
      "\n",
      "Epoch: 114\n",
      "G Loss: 4.308382\n",
      "D Loss 0.815426\n",
      "\n",
      "\n",
      "Epoch: 115\n",
      "G Loss: 2.477990\n",
      "D Loss 0.973702\n",
      "\n",
      "\n",
      "Epoch: 116\n",
      "G Loss: 4.434757\n",
      "D Loss 0.836548\n",
      "\n",
      "\n",
      "Epoch: 117\n",
      "G Loss: 5.463461\n",
      "D Loss 1.025127\n",
      "\n",
      "\n",
      "Epoch: 118\n",
      "G Loss: 4.291644\n",
      "D Loss 0.828587\n",
      "\n",
      "\n",
      "Epoch: 119\n",
      "G Loss: 5.470619\n",
      "D Loss 0.731053\n",
      "\n",
      "\n",
      "Epoch: 120\n",
      "G Loss: 4.722567\n",
      "D Loss 1.048874\n",
      "\n",
      "\n",
      "Epoch: 121\n",
      "G Loss: 0.956454\n",
      "D Loss 1.464293\n",
      "\n",
      "\n",
      "Epoch: 122\n",
      "G Loss: 4.186846\n",
      "D Loss 0.707920\n",
      "\n",
      "\n",
      "Epoch: 123\n",
      "G Loss: 3.502596\n",
      "D Loss 1.104914\n",
      "\n",
      "\n",
      "Epoch: 124\n",
      "G Loss: 5.155548\n",
      "D Loss 0.917658\n",
      "\n",
      "\n",
      "Epoch: 125\n",
      "G Loss: 2.483612\n",
      "D Loss 1.221355\n",
      "\n",
      "\n",
      "Epoch: 126\n",
      "G Loss: 3.151677\n",
      "D Loss 0.964993\n",
      "\n",
      "\n",
      "Epoch: 127\n",
      "G Loss: 4.531419\n",
      "D Loss 0.777349\n",
      "\n",
      "\n",
      "Epoch: 128\n",
      "G Loss: 4.176167\n",
      "D Loss 1.205123\n",
      "\n",
      "\n",
      "Epoch: 129\n",
      "G Loss: 4.160824\n",
      "D Loss 0.757439\n",
      "\n",
      "\n",
      "Epoch: 130\n",
      "G Loss: 3.391816\n",
      "D Loss 0.767964\n",
      "\n",
      "\n",
      "Epoch: 131\n",
      "G Loss: 1.981591\n",
      "D Loss 1.063007\n",
      "\n",
      "\n",
      "Epoch: 132\n",
      "G Loss: 3.353099\n",
      "D Loss 0.818900\n",
      "\n",
      "\n",
      "Epoch: 133\n",
      "G Loss: 4.285607\n",
      "D Loss 1.214617\n",
      "\n",
      "\n",
      "Epoch: 134\n",
      "G Loss: 4.232700\n",
      "D Loss 1.222827\n",
      "\n",
      "\n",
      "Epoch: 135\n",
      "G Loss: 1.729910\n",
      "D Loss 0.899197\n",
      "\n",
      "\n",
      "Epoch: 136\n",
      "G Loss: 5.096196\n",
      "D Loss 0.871278\n",
      "\n",
      "\n",
      "Epoch: 137\n",
      "G Loss: 5.148913\n",
      "D Loss 1.042434\n",
      "\n",
      "\n",
      "Epoch: 138\n",
      "G Loss: 4.118088\n",
      "D Loss 0.877924\n",
      "\n",
      "\n",
      "Epoch: 139\n",
      "G Loss: 3.311571\n",
      "D Loss 0.884758\n",
      "\n",
      "\n",
      "Epoch: 140\n",
      "G Loss: 5.901562\n",
      "D Loss 1.045132\n",
      "\n",
      "\n",
      "Epoch: 141\n",
      "G Loss: 5.544632\n",
      "D Loss 1.005945\n",
      "\n",
      "\n",
      "Epoch: 142\n",
      "G Loss: 1.819247\n",
      "D Loss 0.959693\n",
      "\n",
      "\n",
      "Epoch: 143\n",
      "G Loss: 3.224555\n",
      "D Loss 0.858167\n",
      "\n",
      "\n",
      "Epoch: 144\n",
      "G Loss: 4.525717\n",
      "D Loss 0.921805\n",
      "\n",
      "\n",
      "Epoch: 145\n",
      "G Loss: 3.045380\n",
      "D Loss 0.768177\n",
      "\n",
      "\n",
      "Epoch: 146\n",
      "G Loss: 4.951726\n",
      "D Loss 0.783032\n",
      "\n",
      "\n",
      "Epoch: 147\n",
      "G Loss: 2.365579\n",
      "D Loss 1.069914\n",
      "\n",
      "\n",
      "Epoch: 148\n",
      "G Loss: 4.786933\n",
      "D Loss 0.910901\n",
      "\n",
      "\n",
      "Epoch: 149\n",
      "G Loss: 4.098671\n",
      "D Loss 0.968161\n",
      "\n",
      "\n",
      "Epoch: 150\n",
      "G Loss: 4.862360\n",
      "D Loss 1.054639\n",
      "\n",
      "\n",
      "Epoch: 151\n",
      "G Loss: 2.376788\n",
      "D Loss 1.012129\n",
      "\n",
      "\n",
      "Epoch: 152\n",
      "G Loss: 4.217912\n",
      "D Loss 0.950494\n",
      "\n",
      "\n",
      "Epoch: 153\n",
      "G Loss: 4.720180\n",
      "D Loss 0.841564\n",
      "\n",
      "\n",
      "Epoch: 154\n",
      "G Loss: 3.237909\n",
      "D Loss 0.854287\n",
      "\n",
      "\n",
      "Epoch: 155\n",
      "G Loss: 2.243713\n",
      "D Loss 0.873617\n",
      "\n",
      "\n",
      "Epoch: 156\n",
      "G Loss: 3.440398\n",
      "D Loss 0.753562\n",
      "\n",
      "\n",
      "Epoch: 157\n",
      "G Loss: 5.983919\n",
      "D Loss 1.180246\n",
      "\n",
      "\n",
      "Epoch: 158\n",
      "G Loss: 2.414032\n",
      "D Loss 0.783545\n",
      "\n",
      "\n",
      "Epoch: 159\n",
      "G Loss: 2.007621\n",
      "D Loss 1.237558\n",
      "\n",
      "\n",
      "Epoch: 160\n",
      "G Loss: 3.640455\n",
      "D Loss 0.745454\n",
      "\n",
      "\n",
      "Epoch: 161\n",
      "G Loss: 5.420633\n",
      "D Loss 0.864705\n",
      "\n",
      "\n",
      "Epoch: 162\n",
      "G Loss: 3.536818\n",
      "D Loss 0.806924\n",
      "\n",
      "\n",
      "Epoch: 163\n",
      "G Loss: 2.000809\n",
      "D Loss 0.839561\n",
      "\n",
      "\n",
      "Epoch: 164\n",
      "G Loss: 2.611057\n",
      "D Loss 0.900026\n",
      "\n",
      "\n",
      "Epoch: 165\n",
      "G Loss: 4.159802\n",
      "D Loss 0.771728\n",
      "\n",
      "\n",
      "Epoch: 166\n",
      "G Loss: 5.276928\n",
      "D Loss 0.968857\n",
      "\n",
      "\n",
      "Epoch: 167\n",
      "G Loss: 4.333980\n",
      "D Loss 0.813059\n",
      "\n",
      "\n",
      "Epoch: 168\n",
      "G Loss: 3.852785\n",
      "D Loss 0.835045\n",
      "\n",
      "\n",
      "Epoch: 169\n",
      "G Loss: 5.067721\n",
      "D Loss 0.780303\n",
      "\n",
      "\n",
      "Epoch: 170\n",
      "G Loss: 3.483545\n",
      "D Loss 0.786227\n",
      "\n",
      "\n",
      "Epoch: 171\n",
      "G Loss: 3.485466\n",
      "D Loss 0.904715\n",
      "\n",
      "\n",
      "Epoch: 172\n",
      "G Loss: 5.842479\n",
      "D Loss 0.764173\n",
      "\n",
      "\n",
      "Epoch: 173\n",
      "G Loss: 5.594729\n",
      "D Loss 0.837301\n",
      "\n",
      "\n",
      "Epoch: 174\n",
      "G Loss: 5.354066\n",
      "D Loss 0.750939\n",
      "\n",
      "\n",
      "Epoch: 175\n",
      "G Loss: 6.520111\n",
      "D Loss 0.765014\n",
      "\n",
      "\n",
      "Epoch: 176\n",
      "G Loss: 4.401453\n",
      "D Loss 0.705228\n",
      "\n",
      "\n",
      "Epoch: 177\n",
      "G Loss: 2.286659\n",
      "D Loss 0.843274\n",
      "\n",
      "\n",
      "Epoch: 178\n",
      "G Loss: 4.839024\n",
      "D Loss 0.794941\n",
      "\n",
      "\n",
      "Epoch: 179\n",
      "G Loss: 4.535278\n",
      "D Loss 0.817256\n",
      "\n",
      "\n",
      "Epoch: 180\n",
      "G Loss: 6.382885\n",
      "D Loss 0.831499\n",
      "\n",
      "\n",
      "Epoch: 181\n",
      "G Loss: 2.186591\n",
      "D Loss 1.357378\n",
      "\n",
      "\n",
      "Epoch: 182\n",
      "G Loss: 8.481166\n",
      "D Loss 1.438227\n",
      "\n",
      "\n",
      "Epoch: 183\n",
      "G Loss: 2.175292\n",
      "D Loss 0.887658\n",
      "\n",
      "\n",
      "Epoch: 184\n",
      "G Loss: 3.967474\n",
      "D Loss 1.082721\n",
      "\n",
      "\n",
      "Epoch: 185\n",
      "G Loss: 3.041875\n",
      "D Loss 0.773328\n",
      "\n",
      "\n",
      "Epoch: 186\n",
      "G Loss: 4.337715\n",
      "D Loss 1.102910\n",
      "\n",
      "\n",
      "Epoch: 187\n",
      "G Loss: 5.151159\n",
      "D Loss 0.862711\n",
      "\n",
      "\n",
      "Epoch: 188\n",
      "G Loss: 4.549528\n",
      "D Loss 0.695511\n",
      "\n",
      "\n",
      "Epoch: 189\n",
      "G Loss: 3.904350\n",
      "D Loss 0.788184\n",
      "\n",
      "\n",
      "Epoch: 190\n",
      "G Loss: 3.654653\n",
      "D Loss 0.864158\n",
      "\n",
      "\n",
      "Epoch: 191\n",
      "G Loss: 2.336416\n",
      "D Loss 0.784574\n",
      "\n",
      "\n",
      "Epoch: 192\n",
      "G Loss: 4.309524\n",
      "D Loss 0.691729\n",
      "\n",
      "\n",
      "Epoch: 193\n",
      "G Loss: 3.393838\n",
      "D Loss 0.957327\n",
      "\n",
      "\n",
      "Epoch: 194\n",
      "G Loss: 6.547259\n",
      "D Loss 0.723358\n",
      "\n",
      "\n",
      "Epoch: 195\n",
      "G Loss: 4.416338\n",
      "D Loss 0.845630\n",
      "\n",
      "\n",
      "Epoch: 196\n",
      "G Loss: 5.659397\n",
      "D Loss 0.873101\n",
      "\n",
      "\n",
      "Epoch: 197\n",
      "G Loss: 7.326307\n",
      "D Loss 0.796464\n",
      "\n",
      "\n",
      "Epoch: 198\n",
      "G Loss: 4.125433\n",
      "D Loss 0.706498\n",
      "\n",
      "\n",
      "Epoch: 199\n",
      "G Loss: 3.293443\n",
      "D Loss 0.717064\n",
      "\n",
      "\n",
      "Epoch: 200\n",
      "G Loss: 6.094039\n",
      "D Loss 0.765489\n",
      "\n",
      "\n",
      "Epoch: 201\n",
      "G Loss: 4.666368\n",
      "D Loss 0.661853\n",
      "\n",
      "\n",
      "Epoch: 202\n",
      "G Loss: 3.110152\n",
      "D Loss 0.790116\n",
      "\n",
      "\n",
      "Epoch: 203\n",
      "G Loss: 6.122632\n",
      "D Loss 0.910148\n",
      "\n",
      "\n",
      "Epoch: 204\n",
      "G Loss: 2.578204\n",
      "D Loss 1.167640\n",
      "\n",
      "\n",
      "Epoch: 205\n",
      "G Loss: 6.301455\n",
      "D Loss 0.751119\n",
      "\n",
      "\n",
      "Epoch: 206\n",
      "G Loss: 5.417383\n",
      "D Loss 0.884148\n",
      "\n",
      "\n",
      "Epoch: 207\n",
      "G Loss: 3.133012\n",
      "D Loss 0.969467\n",
      "\n",
      "\n",
      "Epoch: 208\n",
      "G Loss: 2.488619\n",
      "D Loss 1.508593\n",
      "\n",
      "\n",
      "Epoch: 209\n",
      "G Loss: 2.449353\n",
      "D Loss 1.058512\n",
      "\n",
      "\n",
      "Epoch: 210\n",
      "G Loss: 4.827839\n",
      "D Loss 0.779811\n",
      "\n",
      "\n",
      "Epoch: 211\n",
      "G Loss: 2.698134\n",
      "D Loss 0.901252\n",
      "\n",
      "\n",
      "Epoch: 212\n",
      "G Loss: 7.826652\n",
      "D Loss 1.197737\n",
      "\n",
      "\n",
      "Epoch: 213\n",
      "G Loss: 3.008552\n",
      "D Loss 0.826403\n",
      "\n",
      "\n",
      "Epoch: 214\n",
      "G Loss: 1.660958\n",
      "D Loss 1.219269\n",
      "\n",
      "\n",
      "Epoch: 215\n",
      "G Loss: 3.206317\n",
      "D Loss 1.045228\n",
      "\n",
      "\n",
      "Epoch: 216\n",
      "G Loss: 3.779346\n",
      "D Loss 0.787822\n",
      "\n",
      "\n",
      "Epoch: 217\n",
      "G Loss: 3.087864\n",
      "D Loss 0.973626\n",
      "\n",
      "\n",
      "Epoch: 218\n",
      "G Loss: 4.076685\n",
      "D Loss 0.650553\n",
      "\n",
      "\n",
      "Epoch: 219\n",
      "G Loss: 3.593833\n",
      "D Loss 0.730744\n",
      "\n",
      "\n",
      "Epoch: 220\n",
      "G Loss: 3.053463\n",
      "D Loss 0.686731\n",
      "\n",
      "\n",
      "Epoch: 221\n",
      "G Loss: 5.379320\n",
      "D Loss 0.653130\n",
      "\n",
      "\n",
      "Epoch: 222\n",
      "G Loss: 4.689061\n",
      "D Loss 0.693669\n",
      "\n",
      "\n",
      "Epoch: 223\n",
      "G Loss: 5.924290\n",
      "D Loss 0.817410\n",
      "\n",
      "\n",
      "Epoch: 224\n",
      "G Loss: 5.394895\n",
      "D Loss 1.134882\n",
      "\n",
      "\n",
      "Epoch: 225\n",
      "G Loss: 5.727499\n",
      "D Loss 1.013253\n",
      "\n",
      "\n",
      "Epoch: 226\n",
      "G Loss: 5.571179\n",
      "D Loss 0.769581\n",
      "\n",
      "\n",
      "Epoch: 227\n",
      "G Loss: 4.595216\n",
      "D Loss 1.127128\n",
      "\n",
      "\n",
      "Epoch: 228\n",
      "G Loss: 3.970052\n",
      "D Loss 0.722880\n",
      "\n",
      "\n",
      "Epoch: 229\n",
      "G Loss: 3.120612\n",
      "D Loss 0.757806\n",
      "\n",
      "\n",
      "Epoch: 230\n",
      "G Loss: 4.067054\n",
      "D Loss 0.794587\n",
      "\n",
      "\n",
      "Epoch: 231\n",
      "G Loss: 6.764771\n",
      "D Loss 0.695090\n",
      "\n",
      "\n",
      "Epoch: 232\n",
      "G Loss: 7.449790\n",
      "D Loss 0.812380\n",
      "\n",
      "\n",
      "Epoch: 233\n",
      "G Loss: 4.123739\n",
      "D Loss 0.756212\n",
      "\n",
      "\n",
      "Epoch: 234\n",
      "G Loss: 1.790524\n",
      "D Loss 1.275351\n",
      "\n",
      "\n",
      "Epoch: 235\n",
      "G Loss: 3.843190\n",
      "D Loss 0.908442\n",
      "\n",
      "\n",
      "Epoch: 236\n",
      "G Loss: 3.189795\n",
      "D Loss 0.822328\n",
      "\n",
      "\n",
      "Epoch: 237\n",
      "G Loss: 4.233426\n",
      "D Loss 0.746321\n",
      "\n",
      "\n",
      "Epoch: 238\n",
      "G Loss: 1.929243\n",
      "D Loss 0.931387\n",
      "\n",
      "\n",
      "Epoch: 239\n",
      "G Loss: 4.213876\n",
      "D Loss 0.915877\n",
      "\n",
      "\n",
      "Epoch: 240\n",
      "G Loss: 4.125660\n",
      "D Loss 1.271604\n",
      "\n",
      "\n",
      "Epoch: 241\n",
      "G Loss: 4.199936\n",
      "D Loss 0.917681\n",
      "\n",
      "\n",
      "Epoch: 242\n",
      "G Loss: 5.918446\n",
      "D Loss 0.707653\n",
      "\n",
      "\n",
      "Epoch: 243\n",
      "G Loss: 5.271670\n",
      "D Loss 0.852637\n",
      "\n",
      "\n",
      "Epoch: 244\n",
      "G Loss: 4.212721\n",
      "D Loss 1.028714\n",
      "\n",
      "\n",
      "Epoch: 245\n",
      "G Loss: 5.855379\n",
      "D Loss 0.693741\n",
      "\n",
      "\n",
      "Epoch: 246\n",
      "G Loss: 5.363608\n",
      "D Loss 0.792773\n",
      "\n",
      "\n",
      "Epoch: 247\n",
      "G Loss: 1.968246\n",
      "D Loss 0.992337\n",
      "\n",
      "\n",
      "Epoch: 248\n",
      "G Loss: 4.332510\n",
      "D Loss 1.012857\n",
      "\n",
      "\n",
      "Epoch: 249\n",
      "G Loss: 5.957445\n",
      "D Loss 0.707498\n",
      "\n",
      "\n",
      "Epoch: 250\n",
      "G Loss: 4.300972\n",
      "D Loss 0.816229\n",
      "\n",
      "\n",
      "Epoch: 251\n",
      "G Loss: 2.643521\n",
      "D Loss 1.041676\n",
      "\n",
      "\n",
      "Epoch: 252\n",
      "G Loss: 5.596069\n",
      "D Loss 0.778047\n",
      "\n",
      "\n",
      "Epoch: 253\n",
      "G Loss: 4.990431\n",
      "D Loss 0.794367\n",
      "\n",
      "\n",
      "Epoch: 254\n",
      "G Loss: 3.846539\n",
      "D Loss 0.733992\n",
      "\n",
      "\n",
      "Epoch: 255\n",
      "G Loss: 3.185800\n",
      "D Loss 0.877538\n",
      "\n",
      "\n",
      "Epoch: 256\n",
      "G Loss: 3.283810\n",
      "D Loss 0.836059\n",
      "\n",
      "\n",
      "Epoch: 257\n",
      "G Loss: 4.056250\n",
      "D Loss 0.964696\n",
      "\n",
      "\n",
      "Epoch: 258\n",
      "G Loss: 3.762813\n",
      "D Loss 0.829956\n",
      "\n",
      "\n",
      "Epoch: 259\n",
      "G Loss: 8.505750\n",
      "D Loss 0.762576\n",
      "\n",
      "\n",
      "Epoch: 260\n",
      "G Loss: 3.784146\n",
      "D Loss 0.747713\n",
      "\n",
      "\n",
      "Epoch: 261\n",
      "G Loss: 8.520402\n",
      "D Loss 0.797342\n",
      "\n",
      "\n",
      "Epoch: 262\n",
      "G Loss: 7.007901\n",
      "D Loss 0.695747\n",
      "\n",
      "\n",
      "Epoch: 263\n",
      "G Loss: 3.098560\n",
      "D Loss 0.943427\n",
      "\n",
      "\n",
      "Epoch: 264\n",
      "G Loss: 4.687061\n",
      "D Loss 0.712879\n",
      "\n",
      "\n",
      "Epoch: 265\n",
      "G Loss: 1.397653\n",
      "D Loss 1.022278\n",
      "\n",
      "\n",
      "Epoch: 266\n",
      "G Loss: 1.122580\n",
      "D Loss 0.861364\n",
      "\n",
      "\n",
      "Epoch: 267\n",
      "G Loss: 4.106055\n",
      "D Loss 0.855237\n",
      "\n",
      "\n",
      "Epoch: 268\n",
      "G Loss: 3.890088\n",
      "D Loss 0.678767\n",
      "\n",
      "\n",
      "Epoch: 269\n",
      "G Loss: 6.261967\n",
      "D Loss 0.675694\n",
      "\n",
      "\n",
      "Epoch: 270\n",
      "G Loss: 3.076014\n",
      "D Loss 0.882452\n",
      "\n",
      "\n",
      "Epoch: 271\n",
      "G Loss: 4.192894\n",
      "D Loss 0.858700\n",
      "\n",
      "\n",
      "Epoch: 272\n",
      "G Loss: 4.255774\n",
      "D Loss 1.087489\n",
      "\n",
      "\n",
      "Epoch: 273\n",
      "G Loss: 2.183299\n",
      "D Loss 0.830686\n",
      "\n",
      "\n",
      "Epoch: 274\n",
      "G Loss: 4.498943\n",
      "D Loss 0.708075\n",
      "\n",
      "\n",
      "Epoch: 275\n",
      "G Loss: 3.689390\n",
      "D Loss 0.765644\n",
      "\n",
      "\n",
      "Epoch: 276\n",
      "G Loss: 7.255811\n",
      "D Loss 0.708031\n",
      "\n",
      "\n",
      "Epoch: 277\n",
      "G Loss: 6.893180\n",
      "D Loss 1.080656\n",
      "\n",
      "\n",
      "Epoch: 278\n",
      "G Loss: 4.804419\n",
      "D Loss 0.887785\n",
      "\n",
      "\n",
      "Epoch: 279\n",
      "G Loss: 4.669685\n",
      "D Loss 0.747063\n",
      "\n",
      "\n",
      "Epoch: 280\n",
      "G Loss: 4.277853\n",
      "D Loss 0.811281\n",
      "\n",
      "\n",
      "Epoch: 281\n",
      "G Loss: 5.344524\n",
      "D Loss 0.741687\n",
      "\n",
      "\n",
      "Epoch: 282\n",
      "G Loss: 7.647968\n",
      "D Loss 1.207988\n",
      "\n",
      "\n",
      "Epoch: 283\n",
      "G Loss: 6.062987\n",
      "D Loss 0.761828\n",
      "\n",
      "\n",
      "Epoch: 284\n",
      "G Loss: 5.581044\n",
      "D Loss 0.762821\n",
      "\n",
      "\n",
      "Epoch: 285\n",
      "G Loss: 3.417675\n",
      "D Loss 0.762305\n",
      "\n",
      "\n",
      "Epoch: 286\n",
      "G Loss: 4.740263\n",
      "D Loss 0.769600\n",
      "\n",
      "\n",
      "Epoch: 287\n",
      "G Loss: 4.600873\n",
      "D Loss 0.696625\n",
      "\n",
      "\n",
      "Epoch: 288\n",
      "G Loss: 3.842173\n",
      "D Loss 0.818784\n",
      "\n",
      "\n",
      "Epoch: 289\n",
      "G Loss: 5.740875\n",
      "D Loss 0.650425\n",
      "\n",
      "\n",
      "Epoch: 290\n",
      "G Loss: 6.033862\n",
      "D Loss 0.744183\n",
      "\n",
      "\n",
      "Epoch: 291\n",
      "G Loss: 6.176621\n",
      "D Loss 0.850891\n",
      "\n",
      "\n",
      "Epoch: 292\n",
      "G Loss: 5.278911\n",
      "D Loss 0.871589\n",
      "\n",
      "\n",
      "Epoch: 293\n",
      "G Loss: 5.823450\n",
      "D Loss 0.852747\n",
      "\n",
      "\n",
      "Epoch: 294\n",
      "G Loss: 4.654980\n",
      "D Loss 0.810470\n",
      "\n",
      "\n",
      "Epoch: 295\n",
      "G Loss: 4.337255\n",
      "D Loss 0.831949\n",
      "\n",
      "\n",
      "Epoch: 296\n",
      "G Loss: 5.287913\n",
      "D Loss 0.708779\n",
      "\n",
      "\n",
      "Epoch: 297\n",
      "G Loss: 2.101984\n",
      "D Loss 0.798801\n",
      "\n",
      "\n",
      "Epoch: 298\n",
      "G Loss: 4.473309\n",
      "D Loss 0.697863\n",
      "\n",
      "\n",
      "Epoch: 299\n",
      "G Loss: 5.347323\n",
      "D Loss 0.778228\n",
      "\n",
      "\n",
      "Epoch: 300\n",
      "G Loss: 3.759519\n",
      "D Loss 0.778603\n",
      "\n",
      "\n",
      "Epoch: 301\n",
      "G Loss: 9.256310\n",
      "D Loss 1.045465\n",
      "\n",
      "\n",
      "Epoch: 302\n",
      "G Loss: 3.956588\n",
      "D Loss 0.781031\n",
      "\n",
      "\n",
      "Epoch: 303\n",
      "G Loss: 1.544276\n",
      "D Loss 0.919557\n",
      "\n",
      "\n",
      "Epoch: 304\n",
      "G Loss: 5.367062\n",
      "D Loss 0.760341\n",
      "\n",
      "\n",
      "Epoch: 305\n",
      "G Loss: 5.303629\n",
      "D Loss 0.804773\n",
      "\n",
      "\n",
      "Epoch: 306\n",
      "G Loss: 5.687474\n",
      "D Loss 0.795427\n",
      "\n",
      "\n",
      "Epoch: 307\n",
      "G Loss: 3.450401\n",
      "D Loss 0.774641\n",
      "\n",
      "\n",
      "Epoch: 308\n",
      "G Loss: 5.362172\n",
      "D Loss 0.750658\n",
      "\n",
      "\n",
      "Epoch: 309\n",
      "G Loss: 4.498998\n",
      "D Loss 0.767762\n",
      "\n",
      "\n",
      "Epoch: 310\n",
      "G Loss: 2.770389\n",
      "D Loss 1.093495\n",
      "\n",
      "\n",
      "Epoch: 311\n",
      "G Loss: 4.175489\n",
      "D Loss 0.715951\n",
      "\n",
      "\n",
      "Epoch: 312\n",
      "G Loss: 6.297171\n",
      "D Loss 0.893818\n",
      "\n",
      "\n",
      "Epoch: 313\n",
      "G Loss: 5.465684\n",
      "D Loss 0.897528\n",
      "\n",
      "\n",
      "Epoch: 314\n",
      "G Loss: 5.904944\n",
      "D Loss 0.665455\n",
      "\n",
      "\n",
      "Epoch: 315\n",
      "G Loss: 6.333860\n",
      "D Loss 0.833617\n",
      "\n",
      "\n",
      "Epoch: 316\n",
      "G Loss: 5.119987\n",
      "D Loss 0.715089\n",
      "\n",
      "\n",
      "Epoch: 317\n",
      "G Loss: 4.792195\n",
      "D Loss 0.831254\n",
      "\n",
      "\n",
      "Epoch: 318\n",
      "G Loss: 2.861171\n",
      "D Loss 0.757032\n",
      "\n",
      "\n",
      "Epoch: 319\n",
      "G Loss: 5.471144\n",
      "D Loss 1.051395\n",
      "\n",
      "\n",
      "Epoch: 320\n",
      "G Loss: 3.619682\n",
      "D Loss 0.928109\n",
      "\n",
      "\n",
      "Epoch: 321\n",
      "G Loss: 7.430799\n",
      "D Loss 0.867405\n",
      "\n",
      "\n",
      "Epoch: 322\n",
      "G Loss: 3.141465\n",
      "D Loss 0.841852\n",
      "\n",
      "\n",
      "Epoch: 323\n",
      "G Loss: 3.726333\n",
      "D Loss 0.763062\n",
      "\n",
      "\n",
      "Epoch: 324\n",
      "G Loss: 6.817964\n",
      "D Loss 1.278167\n",
      "\n",
      "\n",
      "Epoch: 325\n",
      "G Loss: 6.876294\n",
      "D Loss 0.788459\n",
      "\n",
      "\n",
      "Epoch: 326\n",
      "G Loss: 3.775369\n",
      "D Loss 0.732539\n",
      "\n",
      "\n",
      "Epoch: 327\n",
      "G Loss: 6.918591\n",
      "D Loss 0.863608\n",
      "\n",
      "\n",
      "Epoch: 328\n",
      "G Loss: 2.529057\n",
      "D Loss 0.776567\n",
      "\n",
      "\n",
      "Epoch: 329\n",
      "G Loss: 4.640148\n",
      "D Loss 0.744486\n",
      "\n",
      "\n",
      "Epoch: 330\n",
      "G Loss: 3.419027\n",
      "D Loss 0.699093\n",
      "\n",
      "\n",
      "Epoch: 331\n",
      "G Loss: 5.380304\n",
      "D Loss 0.836949\n",
      "\n",
      "\n",
      "Epoch: 332\n",
      "G Loss: 5.078128\n",
      "D Loss 0.904919\n",
      "\n",
      "\n",
      "Epoch: 333\n",
      "G Loss: 3.775757\n",
      "D Loss 0.874439\n",
      "\n",
      "\n",
      "Epoch: 334\n",
      "G Loss: 4.135417\n",
      "D Loss 0.856433\n",
      "\n",
      "\n",
      "Epoch: 335\n",
      "G Loss: 9.241059\n",
      "D Loss 1.211896\n",
      "\n",
      "\n",
      "Epoch: 336\n",
      "G Loss: 3.287405\n",
      "D Loss 0.754560\n",
      "\n",
      "\n",
      "Epoch: 337\n",
      "G Loss: 4.215881\n",
      "D Loss 0.895958\n",
      "\n",
      "\n",
      "Epoch: 338\n",
      "G Loss: 8.269192\n",
      "D Loss 0.782507\n",
      "\n",
      "\n",
      "Epoch: 339\n",
      "G Loss: 4.111752\n",
      "D Loss 0.816770\n",
      "\n",
      "\n",
      "Epoch: 340\n",
      "G Loss: 6.518242\n",
      "D Loss 0.778956\n",
      "\n",
      "\n",
      "Epoch: 341\n",
      "G Loss: 4.380151\n",
      "D Loss 0.879807\n",
      "\n",
      "\n",
      "Epoch: 342\n",
      "G Loss: 3.284000\n",
      "D Loss 0.715215\n",
      "\n",
      "\n",
      "Epoch: 343\n",
      "G Loss: 6.887059\n",
      "D Loss 0.748244\n",
      "\n",
      "\n",
      "Epoch: 344\n",
      "G Loss: 4.560287\n",
      "D Loss 0.776585\n",
      "\n",
      "\n",
      "Epoch: 345\n",
      "G Loss: 5.710832\n",
      "D Loss 0.749633\n",
      "\n",
      "\n",
      "Epoch: 346\n",
      "G Loss: 4.528553\n",
      "D Loss 0.787879\n",
      "\n",
      "\n",
      "Epoch: 347\n",
      "G Loss: 3.836888\n",
      "D Loss 0.718897\n",
      "\n",
      "\n",
      "Epoch: 348\n",
      "G Loss: 4.065302\n",
      "D Loss 0.828255\n",
      "\n",
      "\n",
      "Epoch: 349\n",
      "G Loss: 5.514077\n",
      "D Loss 0.766891\n",
      "\n",
      "\n",
      "Epoch: 350\n",
      "G Loss: 5.354184\n",
      "D Loss 0.745371\n",
      "\n",
      "\n",
      "Epoch: 351\n",
      "G Loss: 4.650967\n",
      "D Loss 0.930010\n",
      "\n",
      "\n",
      "Epoch: 352\n",
      "G Loss: 6.080502\n",
      "D Loss 0.709986\n",
      "\n",
      "\n",
      "Epoch: 353\n",
      "G Loss: 3.220780\n",
      "D Loss 0.679620\n",
      "\n",
      "\n",
      "Epoch: 354\n",
      "G Loss: 3.027942\n",
      "D Loss 0.773139\n",
      "\n",
      "\n",
      "Epoch: 355\n",
      "G Loss: 4.489517\n",
      "D Loss 0.793589\n",
      "\n",
      "\n",
      "Epoch: 356\n",
      "G Loss: 3.422879\n",
      "D Loss 0.953496\n",
      "\n",
      "\n",
      "Epoch: 357\n",
      "G Loss: 5.527593\n",
      "D Loss 0.847918\n",
      "\n",
      "\n",
      "Epoch: 358\n",
      "G Loss: 3.984075\n",
      "D Loss 0.808616\n",
      "\n",
      "\n",
      "Epoch: 359\n",
      "G Loss: 2.561199\n",
      "D Loss 1.242063\n",
      "\n",
      "\n",
      "Epoch: 360\n",
      "G Loss: 4.216789\n",
      "D Loss 0.707005\n",
      "\n",
      "\n",
      "Epoch: 361\n",
      "G Loss: 4.281843\n",
      "D Loss 0.749925\n",
      "\n",
      "\n",
      "Epoch: 362\n",
      "G Loss: 7.333633\n",
      "D Loss 1.369088\n",
      "\n",
      "\n",
      "Epoch: 363\n",
      "G Loss: 7.394261\n",
      "D Loss 0.779664\n",
      "\n",
      "\n",
      "Epoch: 364\n",
      "G Loss: 3.362480\n",
      "D Loss 0.782417\n",
      "\n",
      "\n",
      "Epoch: 365\n",
      "G Loss: 2.805647\n",
      "D Loss 1.141617\n",
      "\n",
      "\n",
      "Epoch: 366\n",
      "G Loss: 3.152323\n",
      "D Loss 0.717815\n",
      "\n",
      "\n",
      "Epoch: 367\n",
      "G Loss: 3.217631\n",
      "D Loss 0.879283\n",
      "\n",
      "\n",
      "Epoch: 368\n"
     ]
    }
   ],
   "source": [
    "def train(net_g, net_d, epochs, batch_size, lr, log_every_n=50):\n",
    "    \"\"\"\n",
    "    Training a network\n",
    "    :param net: Network for training\n",
    "    :param epochs: Number of epochs in total.\n",
    "    :param batch_size: Batch size for training.\n",
    "    \"\"\"\n",
    "    print('==> Preparing data..')\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "\n",
    "    ])\n",
    "    best_acc = 0  # best test accuracy\n",
    "    start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=0)\n",
    "\n",
    "    d_criterion = nn.BCEWithLogitsLoss()\n",
    "    aux_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    d_optmizer = optim.Adam(net_d.parameters(), d_lr, (.5, .999))\n",
    "    g_optimizer = optim.Adam(net_g.parameters(), g_lr, (.5, .999))\n",
    "\n",
    "    global_steps = 0\n",
    "    start = time.time()\n",
    "\n",
    "    real_targ = torch.FloatTensor(batch_size, 1).fill_(1.0).to(device)\n",
    "    fake_targ = torch.FloatTensor(batch_size, 1).fill_(0.0).to(device)\n",
    "    dlosses = []\n",
    "    glosses = []\n",
    "    \n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        \"\"\"\n",
    "        Start the training code.\n",
    "        \"\"\"\n",
    "        print('\\nEpoch: %d' % epoch)\n",
    "        net_g.train()\n",
    "        net_d.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            g_optimizer.zero_grad()\n",
    "\n",
    "            # train generator\n",
    "            noise = np.random.normal(0, 1, (batch_size, nz))\n",
    "            np_gen_label = np.random.randint(0, num_classes, batch_size)\n",
    "            onehot = np.zeros((batch_size, num_classes))\n",
    "            onehot[np.arange(batch_size), np_gen_label] = 1\n",
    "            z = np.concatenate((noise, onehot), axis=1)\n",
    "            z = torch.from_numpy(z).float().to(device)\n",
    "\n",
    "            gen_label = torch.from_numpy(np_gen_label).long().to(device)\n",
    "\n",
    "\n",
    "            gen_im = net_g(z)\n",
    "            gen_im_validity, gen_im_class = net_d(gen_im)\n",
    "            g_tricks_d = d_criterion(gen_im_validity, real_targ)\n",
    "            g_label_correct = aux_criterion(gen_im_class, gen_label)\n",
    "            g_loss = g_tricks_d + g_label_correct\n",
    "            g_loss.backward()\n",
    "\n",
    "            g_optimizer.step()\n",
    "\n",
    "            # train discriminator\n",
    "            d_optmizer.zero_grad()\n",
    "\n",
    "            gen_label = torch.from_numpy(np_gen_label).long().to(device)\n",
    "\n",
    "            gen_im = net_g(z)\n",
    "            gen_im_validity, gen_im_class = net_d(gen_im)\n",
    "\n",
    "#             print(gen_im_validity, fake_targ)\n",
    "#             print(aux_criterion(gen_im_class, gen_label))\n",
    "            d_fake_loss = d_criterion(gen_im_validity, fake_targ) + aux_criterion(gen_im_class, gen_label)\n",
    "\n",
    "            real_im_validity, real_im_class = net_d(inputs)\n",
    "#             print(d_criterion(real_im_validity, real_targ))\n",
    "#             print(aux_criterion(real_im_class, targets))\n",
    "            \n",
    "\n",
    "            d_real_loss = d_criterion(real_im_validity, real_targ) + aux_criterion(real_im_class, targets)\n",
    "\n",
    "        \n",
    "            if d_fake_loss.item()<0:\n",
    "                print(\"issue:\")\n",
    "                print(d_criterion(gen_im_validity, fake_targ))\n",
    "                print(aux_criterion(gen_im_class, gen_label))\n",
    "                print(gen_im_validity)\n",
    "                print(gen_im_class)\n",
    "            d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "\n",
    "            d_loss.backward()\n",
    "\n",
    "            d_optmizer.step()\n",
    "        if epoch %5==0:\n",
    "            sample_image(5, epoch, net_g)\n",
    "\n",
    "#             if global_steps % log_every_n == 0:\n",
    "#                 end = time.time()\n",
    "#                 num_examples_per_second = log_every_n * batch_size / (end - start)\n",
    "        print(\"G Loss: %f\\nD Loss %f\"\n",
    "              % (g_loss.item(), d_loss.item()))\n",
    "        print(\"\")\n",
    "        if epoch%50==0:\n",
    "            torch.save(net_g.state_dict(), '%s/netG_epoch_%d.pth' % ('models', epoch))\n",
    "            torch.save(net_d.state_dict(), '%s/netD_epoch_%d.pth' % ('models', epoch))\n",
    "#                 start = time.time()\n",
    "        glosses.append(g_loss.item())\n",
    "        dlosses.append(d_loss.item())\n",
    "    return net_g, net_d, dlosses, glosses\n",
    "\n",
    "net_g = netG(nz+num_classes).to(device)\n",
    "net_g.apply(weights_init)\n",
    "net_d = netD(num_classes).to(device)\n",
    "net_d.apply(weights_init)\n",
    "\n",
    "net_g, net_d, dlosses, glosses = train(net_g, net_d, 500, 500, 0.0002, log_every_n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net_g.state_dict(), '%s/netG_epoch_500.pth' % ('models'))\n",
    "torch.save(net_d.state_dict(), '%s/netD_epoch_500.pth' % ('models'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(dlosses)\n",
    "plt.show()\n",
    "plt.plot(glosses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_image(generator, f_name):\n",
    "    \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\"\n",
    "    # Sample noise\n",
    "    batch_size = 30\n",
    "    noise = np.random.normal(0, 1, (batch_size, nz))\n",
    "    np_gen_label = np.concatenate([np.arange(10) for i in range(3)])\n",
    "    onehot = np.zeros((batch_size, num_classes))\n",
    "    onehot[np.arange(batch_size), np_gen_label] = 1\n",
    "    z = np.concatenate((noise, onehot), axis=1)\n",
    "    z = torch.from_numpy(z).float().to(device)\n",
    "    \n",
    "    \n",
    "    gen_imgs = generator(z)\n",
    "    save_image(gen_imgs.data, \"summary_images/\"+f_name+'.png', nrow=10, normalize=True)\n",
    "\n",
    "#     npimg = gen_imgs.cpu().detach().numpy()\n",
    "#     for i in range(30):\n",
    "#         print(npimg[i])\n",
    "#         plt.subplot(3,10,i+1)\n",
    "#         plt.imshow(np.transpose(npimg[i], (1,2,0)), interpolation='nearest')\n",
    "#     plt.show()\n",
    "# test_image(net_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    epoch = str(int(50*i))\n",
    "    PATH = 'models/netG_epoch_'+epoch+'.pth'\n",
    "    \n",
    "    net_g.load_state_dict(torch.load(PATH))\n",
    "    net_g.eval()\n",
    "    \n",
    "    test_image(net_g, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
