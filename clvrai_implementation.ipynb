{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class _netG_CIFAR10(nn.Module):\n",
    "    def __init__(self, ngpu, nz):\n",
    "        super(_netG_CIFAR10, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.nz = nz\n",
    "\n",
    "        # first linear layer\n",
    "        self.fc1 = nn.Linear(110, 384)\n",
    "        # Transposed Convolution 2\n",
    "        self.tconv2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(384, 192, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        # Transposed Convolution 3\n",
    "        self.tconv3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(192, 96, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        # Transposed Convolution 4\n",
    "        self.tconv4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(96, 48, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(48),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        # Transposed Convolution 4\n",
    "        self.tconv5 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(48, 3, 4, 2, 1, bias=False),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "            input = input.view(-1, self.nz)\n",
    "            fc1 = nn.parallel.data_parallel(self.fc1, input, range(self.ngpu))\n",
    "            fc1 = fc1.view(-1, 384, 1, 1)\n",
    "            tconv2 = nn.parallel.data_parallel(self.tconv2, fc1, range(self.ngpu))\n",
    "            tconv3 = nn.parallel.data_parallel(self.tconv3, tconv2, range(self.ngpu))\n",
    "            tconv4 = nn.parallel.data_parallel(self.tconv4, tconv3, range(self.ngpu))\n",
    "            tconv5 = nn.parallel.data_parallel(self.tconv5, tconv4, range(self.ngpu))\n",
    "            output = tconv5\n",
    "        else:\n",
    "            input = input.view(-1, self.nz)\n",
    "            fc1 = self.fc1(input)\n",
    "            fc1 = fc1.view(-1, 384, 1, 1)\n",
    "            tconv2 = self.tconv2(fc1)\n",
    "            tconv3 = self.tconv3(tconv2)\n",
    "            tconv4 = self.tconv4(tconv3)\n",
    "            tconv5 = self.tconv5(tconv4)\n",
    "            output = tconv5\n",
    "        return output\n",
    "\n",
    "\n",
    "class _netD_CIFAR10(nn.Module):\n",
    "    def __init__(self, ngpu, num_classes=10):\n",
    "        super(_netD_CIFAR10, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "\n",
    "        # Convolution 1\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.5, inplace=False),\n",
    "        )\n",
    "        # Convolution 2\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.5, inplace=False),\n",
    "        )\n",
    "        # Convolution 3\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.5, inplace=False),\n",
    "        )\n",
    "        # Convolution 4\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.5, inplace=False),\n",
    "        )\n",
    "        # Convolution 5\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 3, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.5, inplace=False),\n",
    "        )\n",
    "        # Convolution 6\n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.5, inplace=False),\n",
    "        )\n",
    "        # discriminator fc\n",
    "        self.fc_dis = nn.Linear(4*4*512, 1)\n",
    "        # aux-classifier fc\n",
    "        self.fc_aux = nn.Linear(4*4*512, num_classes)\n",
    "        # softmax and sigmoid\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input):\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "            conv1 = nn.parallel.data_parallel(self.conv1, input, range(self.ngpu))\n",
    "            conv2 = nn.parallel.data_parallel(self.conv2, conv1, range(self.ngpu))\n",
    "            conv3 = nn.parallel.data_parallel(self.conv3, conv2, range(self.ngpu))\n",
    "            conv4 = nn.parallel.data_parallel(self.conv4, conv3, range(self.ngpu))\n",
    "            conv5 = nn.parallel.data_parallel(self.conv5, conv4, range(self.ngpu))\n",
    "            conv6 = nn.parallel.data_parallel(self.conv6, conv5, range(self.ngpu))\n",
    "            flat6 = conv6.view(-1, 4*4*512)\n",
    "            fc_dis = nn.parallel.data_parallel(self.fc_dis, flat6, range(self.ngpu))\n",
    "            fc_aux = nn.parallel.data_parallel(self.fc_aux, flat6, range(self.ngpu))\n",
    "        else:\n",
    "            conv1 = self.conv1(input)\n",
    "            conv2 = self.conv2(conv1)\n",
    "            conv3 = self.conv3(conv2)\n",
    "            conv4 = self.conv4(conv3)\n",
    "            conv5 = self.conv5(conv4)\n",
    "            conv6 = self.conv6(conv5)\n",
    "            flat6 = conv6.view(-1, 4*4*512)\n",
    "            fc_dis = self.fc_dis(flat6)\n",
    "            fc_aux = self.fc_aux(flat6)\n",
    "        classes = self.softmax(fc_aux)\n",
    "        realfake = self.sigmoid(fc_dis).view(-1, 1).squeeze(1)\n",
    "        return realfake, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:133: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/100][199/200] Loss_D: 0.9386 (0.9462) Loss_G: 1.2028 (1.2321) D(x): 0.5343 D(G(z)): 0.4253 / 0.3092 Acc: 25.2000 (18.6900)\n",
      "Label for eval = [7 9 7 2 6 4 9 4 7 4 4 1 5 4 6 5 8 1 6 4 2 8 6 5 8 0 8 4 3 9 0 0 3 5 7 6 1\n",
      " 6 7 2 7 9 6 4 4 1 0 1 3 7 4 1 0 5 7 7 9 5 9 0 7 4 9 5 1 7 8 7 4 4 6 3 2 7\n",
      " 2 1 8 8 1 1 2 8 7 3 8 9 9 8 5 4 4 6 4 2 8 2 1 6 1 5 3 1 2 2 9 9 7 4 4 0 4\n",
      " 6 2 7 9 8 0 2 4 8 4 8 5 8 6 9 4 9 1 6 3 7 6 8 5 2 7 4 0 5 7 2 6 3 5 4 9 0\n",
      " 6 9 9 0 8 3 1 3 8 4 3 2 4 8 4 0 0 2 8 7 3 1 9 4 4 4 2 2 9 2 4 1 9 7 4 2 6\n",
      " 8 6 4 5 0 1 4 9 7 1 5 2 5 4 7 5 6 8 6 5 9 3 9 6 0 1 9 2 0 4 5 3 5 6 8 5 7\n",
      " 2 5 0 3 0 3 8 5 8 9 1 3 2 4 2 6 5 6 3 3 9 7 8 8 7 7 2 5]\n",
      "[1/100][199/200] Loss_D: 0.7978 (0.9461) Loss_G: 0.7947 (1.0722) D(x): 0.5791 D(G(z)): 0.4368 / 0.4110 Acc: 23.6000 (21.7500)\n",
      "[2/100][199/200] Loss_D: 0.6132 (0.8595) Loss_G: 0.4900 (1.0223) D(x): 0.5383 D(G(z)): 0.4720 / 0.4134 Acc: 28.8000 (23.7467)\n",
      "[3/100][199/200] Loss_D: 0.5291 (0.7463) Loss_G: 0.3164 (0.9313) D(x): 0.5105 D(G(z)): 0.4348 / 0.4529 Acc: 28.8000 (24.8250)\n",
      "[4/100][199/200] Loss_D: -0.1299 (0.6276) Loss_G: 0.9078 (0.9064) D(x): 0.5711 D(G(z)): 0.1970 / 0.2527 Acc: 29.2000 (25.4864)\n",
      "[5/100][199/200] Loss_D: 0.2508 (0.5582) Loss_G: 0.5869 (0.8953) D(x): 0.5777 D(G(z)): 0.4458 / 0.3231 Acc: 30.8000 (26.0720)\n",
      "Label for eval = [7 9 7 2 6 4 9 4 7 4 4 1 5 4 6 5 8 1 6 4 2 8 6 5 8 0 8 4 3 9 0 0 3 5 7 6 1\n",
      " 6 7 2 7 9 6 4 4 1 0 1 3 7 4 1 0 5 7 7 9 5 9 0 7 4 9 5 1 7 8 7 4 4 6 3 2 7\n",
      " 2 1 8 8 1 1 2 8 7 3 8 9 9 8 5 4 4 6 4 2 8 2 1 6 1 5 3 1 2 2 9 9 7 4 4 0 4\n",
      " 6 2 7 9 8 0 2 4 8 4 8 5 8 6 9 4 9 1 6 3 7 6 8 5 2 7 4 0 5 7 2 6 3 5 4 9 0\n",
      " 6 9 9 0 8 3 1 3 8 4 3 2 4 8 4 0 0 2 8 7 3 1 9 4 4 4 2 2 9 2 4 1 9 7 4 2 6\n",
      " 8 6 4 5 0 1 4 9 7 1 5 2 5 4 7 5 6 8 6 5 9 3 9 6 0 1 9 2 0 4 5 3 5 6 8 5 7\n",
      " 2 5 0 3 0 3 8 5 8 9 1 3 2 4 2 6 5 6 3 3 9 7 8 8 7 7 2 5]\n",
      "[6/100][199/200] Loss_D: 0.1125 (0.4963) Loss_G: 0.2887 (0.8625) D(x): 0.4974 D(G(z)): 0.3283 / 0.3853 Acc: 32.8000 (26.5549)\n",
      "[7/100][199/200] Loss_D: 0.0869 (0.4478) Loss_G: 0.7873 (0.8246) D(x): 0.6432 D(G(z)): 0.4566 / 0.2357 Acc: 28.4000 (27.0622)\n",
      "[8/100][199/200] Loss_D: 0.2086 (0.4081) Loss_G: 0.6388 (0.7794) D(x): 0.6315 D(G(z)): 0.4892 / 0.2883 Acc: 25.2000 (27.5556)\n",
      "[9/100][199/200] Loss_D: -0.0375 (0.3758) Loss_G: 0.8463 (0.7421) D(x): 0.6198 D(G(z)): 0.3998 / 0.2259 Acc: 28.8000 (28.0724)\n",
      "[10/100][199/200] Loss_D: 0.0561 (0.3482) Loss_G: 0.0189 (0.7135) D(x): 0.4965 D(G(z)): 0.3794 / 0.4464 Acc: 36.4000 (28.4533)\n",
      "Label for eval = [7 9 7 2 6 4 9 4 7 4 4 1 5 4 6 5 8 1 6 4 2 8 6 5 8 0 8 4 3 9 0 0 3 5 7 6 1\n",
      " 6 7 2 7 9 6 4 4 1 0 1 3 7 4 1 0 5 7 7 9 5 9 0 7 4 9 5 1 7 8 7 4 4 6 3 2 7\n",
      " 2 1 8 8 1 1 2 8 7 3 8 9 9 8 5 4 4 6 4 2 8 2 1 6 1 5 3 1 2 2 9 9 7 4 4 0 4\n",
      " 6 2 7 9 8 0 2 4 8 4 8 5 8 6 9 4 9 1 6 3 7 6 8 5 2 7 4 0 5 7 2 6 3 5 4 9 0\n",
      " 6 9 9 0 8 3 1 3 8 4 3 2 4 8 4 0 0 2 8 7 3 1 9 4 4 4 2 2 9 2 4 1 9 7 4 2 6\n",
      " 8 6 4 5 0 1 4 9 7 1 5 2 5 4 7 5 6 8 6 5 9 3 9 6 0 1 9 2 0 4 5 3 5 6 8 5 7\n",
      " 2 5 0 3 0 3 8 5 8 9 1 3 2 4 2 6 5 6 3 3 9 7 8 8 7 7 2 5]\n",
      "[11/100][199/200] Loss_D: -0.1400 (0.3241) Loss_G: 0.5033 (0.6824) D(x): 0.5708 D(G(z)): 0.3522 / 0.3028 Acc: 35.2000 (28.8480)\n",
      "[12/100][199/200] Loss_D: -0.0088 (0.3001) Loss_G: 0.1324 (0.6478) D(x): 0.5170 D(G(z)): 0.4345 / 0.3743 Acc: 39.6000 (29.2455)\n",
      "[13/100][199/200] Loss_D: 0.0664 (0.2752) Loss_G: 0.2877 (0.6146) D(x): 0.5987 D(G(z)): 0.5070 / 0.3257 Acc: 31.6000 (29.6203)\n",
      "[14/100][199/200] Loss_D: -0.0606 (0.2503) Loss_G: 0.0547 (0.5899) D(x): 0.5334 D(G(z)): 0.3776 / 0.4041 Acc: 33.2000 (30.0336)\n",
      "[15/100][199/200] Loss_D: -0.0982 (0.2317) Loss_G: -0.0829 (0.5625) D(x): 0.4746 D(G(z)): 0.3247 / 0.4526 Acc: 41.2000 (30.4239)\n",
      "Label for eval = [7 9 7 2 6 4 9 4 7 4 4 1 5 4 6 5 8 1 6 4 2 8 6 5 8 0 8 4 3 9 0 0 3 5 7 6 1\n",
      " 6 7 2 7 9 6 4 4 1 0 1 3 7 4 1 0 5 7 7 9 5 9 0 7 4 9 5 1 7 8 7 4 4 6 3 2 7\n",
      " 2 1 8 8 1 1 2 8 7 3 8 9 9 8 5 4 4 6 4 2 8 2 1 6 1 5 3 1 2 2 9 9 7 4 4 0 4\n",
      " 6 2 7 9 8 0 2 4 8 4 8 5 8 6 9 4 9 1 6 3 7 6 8 5 2 7 4 0 5 7 2 6 3 5 4 9 0\n",
      " 6 9 9 0 8 3 1 3 8 4 3 2 4 8 4 0 0 2 8 7 3 1 9 4 4 4 2 2 9 2 4 1 9 7 4 2 6\n",
      " 8 6 4 5 0 1 4 9 7 1 5 2 5 4 7 5 6 8 6 5 9 3 9 6 0 1 9 2 0 4 5 3 5 6 8 5 7\n",
      " 2 5 0 3 0 3 8 5 8 9 1 3 2 4 2 6 5 6 3 3 9 7 8 8 7 7 2 5]\n",
      "[16/100][199/200] Loss_D: -0.2078 (0.2142) Loss_G: 0.1907 (0.5367) D(x): 0.6018 D(G(z)): 0.4149 / 0.3517 Acc: 36.8000 (30.8175)\n",
      "[17/100][199/200] Loss_D: -0.1735 (0.1993) Loss_G: -0.0635 (0.5149) D(x): 0.5595 D(G(z)): 0.4101 / 0.4415 Acc: 42.0000 (31.1810)\n",
      "[18/100][199/200] Loss_D: -0.1448 (0.1854) Loss_G: -0.0270 (0.4918) D(x): 0.5129 D(G(z)): 0.3954 / 0.4346 Acc: 46.8000 (31.5717)\n",
      "[19/100][199/200] Loss_D: 0.0257 (0.1738) Loss_G: 0.0401 (0.4701) D(x): 0.5414 D(G(z)): 0.4682 / 0.3939 Acc: 33.2000 (31.9417)\n",
      "[20/100][199/200] Loss_D: -0.0420 (0.1626) Loss_G: 0.1068 (0.4487) D(x): 0.5119 D(G(z)): 0.4484 / 0.3852 Acc: 43.6000 (32.2932)\n",
      "Label for eval = [7 9 7 2 6 4 9 4 7 4 4 1 5 4 6 5 8 1 6 4 2 8 6 5 8 0 8 4 3 9 0 0 3 5 7 6 1\n",
      " 6 7 2 7 9 6 4 4 1 0 1 3 7 4 1 0 5 7 7 9 5 9 0 7 4 9 5 1 7 8 7 4 4 6 3 2 7\n",
      " 2 1 8 8 1 1 2 8 7 3 8 9 9 8 5 4 4 6 4 2 8 2 1 6 1 5 3 1 2 2 9 9 7 4 4 0 4\n",
      " 6 2 7 9 8 0 2 4 8 4 8 5 8 6 9 4 9 1 6 3 7 6 8 5 2 7 4 0 5 7 2 6 3 5 4 9 0\n",
      " 6 9 9 0 8 3 1 3 8 4 3 2 4 8 4 0 0 2 8 7 3 1 9 4 4 4 2 2 9 2 4 1 9 7 4 2 6\n",
      " 8 6 4 5 0 1 4 9 7 1 5 2 5 4 7 5 6 8 6 5 9 3 9 6 0 1 9 2 0 4 5 3 5 6 8 5 7\n",
      " 2 5 0 3 0 3 8 5 8 9 1 3 2 4 2 6 5 6 3 3 9 7 8 8 7 7 2 5]\n",
      "[21/100][199/200] Loss_D: -0.0068 (0.1527) Loss_G: -0.2415 (0.4310) D(x): 0.5166 D(G(z)): 0.4677 / 0.5069 Acc: 42.8000 (32.6327)\n",
      "[22/100][199/200] Loss_D: 0.0090 (0.1428) Loss_G: -0.0979 (0.4117) D(x): 0.5523 D(G(z)): 0.4873 / 0.4491 Acc: 38.4000 (32.9802)\n",
      "[23/100][199/200] Loss_D: 0.1117 (0.1342) Loss_G: -0.2682 (0.3944) D(x): 0.4442 D(G(z)): 0.4332 / 0.5251 Acc: 43.6000 (33.3097)\n",
      "[24/100][199/200] Loss_D: 0.0951 (0.1259) Loss_G: -0.1374 (0.3782) D(x): 0.5122 D(G(z)): 0.5030 / 0.4713 Acc: 41.2000 (33.6281)\n",
      "[25/100][199/200] Loss_D: -0.1575 (0.1180) Loss_G: 0.0477 (0.3636) D(x): 0.5002 D(G(z)): 0.3851 / 0.4019 Acc: 46.0000 (33.9296)\n",
      "Label for eval = [7 9 7 2 6 4 9 4 7 4 4 1 5 4 6 5 8 1 6 4 2 8 6 5 8 0 8 4 3 9 0 0 3 5 7 6 1\n",
      " 6 7 2 7 9 6 4 4 1 0 1 3 7 4 1 0 5 7 7 9 5 9 0 7 4 9 5 1 7 8 7 4 4 6 3 2 7\n",
      " 2 1 8 8 1 1 2 8 7 3 8 9 9 8 5 4 4 6 4 2 8 2 1 6 1 5 3 1 2 2 9 9 7 4 4 0 4\n",
      " 6 2 7 9 8 0 2 4 8 4 8 5 8 6 9 4 9 1 6 3 7 6 8 5 2 7 4 0 5 7 2 6 3 5 4 9 0\n",
      " 6 9 9 0 8 3 1 3 8 4 3 2 4 8 4 0 0 2 8 7 3 1 9 4 4 4 2 2 9 2 4 1 9 7 4 2 6\n",
      " 8 6 4 5 0 1 4 9 7 1 5 2 5 4 7 5 6 8 6 5 9 3 9 6 0 1 9 2 0 4 5 3 5 6 8 5 7\n",
      " 2 5 0 3 0 3 8 5 8 9 1 3 2 4 2 6 5 6 3 3 9 7 8 8 7 7 2 5]\n",
      "[26/100][199/200] Loss_D: -0.0697 (0.1113) Loss_G: 0.0530 (0.3491) D(x): 0.5868 D(G(z)): 0.5002 / 0.3964 Acc: 42.4000 (34.2230)\n",
      "[27/100][199/200] Loss_D: -0.1196 (0.1042) Loss_G: -0.1331 (0.3365) D(x): 0.5267 D(G(z)): 0.4211 / 0.4600 Acc: 42.0000 (34.5039)\n",
      "[28/100][199/200] Loss_D: -0.0197 (0.0980) Loss_G: -0.0186 (0.3239) D(x): 0.5366 D(G(z)): 0.4704 / 0.4240 Acc: 42.4000 (34.7637)\n",
      "[29/100][199/200] Loss_D: -0.0797 (0.0924) Loss_G: -0.0590 (0.3119) D(x): 0.5862 D(G(z)): 0.4848 / 0.4365 Acc: 41.2000 (35.0165)\n",
      "[30/100][199/200] Loss_D: -0.0597 (0.0865) Loss_G: -0.0346 (0.2998) D(x): 0.5596 D(G(z)): 0.4570 / 0.4183 Acc: 38.0000 (35.2716)\n",
      "Label for eval = [7 9 7 2 6 4 9 4 7 4 4 1 5 4 6 5 8 1 6 4 2 8 6 5 8 0 8 4 3 9 0 0 3 5 7 6 1\n",
      " 6 7 2 7 9 6 4 4 1 0 1 3 7 4 1 0 5 7 7 9 5 9 0 7 4 9 5 1 7 8 7 4 4 6 3 2 7\n",
      " 2 1 8 8 1 1 2 8 7 3 8 9 9 8 5 4 4 6 4 2 8 2 1 6 1 5 3 1 2 2 9 9 7 4 4 0 4\n",
      " 6 2 7 9 8 0 2 4 8 4 8 5 8 6 9 4 9 1 6 3 7 6 8 5 2 7 4 0 5 7 2 6 3 5 4 9 0\n",
      " 6 9 9 0 8 3 1 3 8 4 3 2 4 8 4 0 0 2 8 7 3 1 9 4 4 4 2 2 9 2 4 1 9 7 4 2 6\n",
      " 8 6 4 5 0 1 4 9 7 1 5 2 5 4 7 5 6 8 6 5 9 3 9 6 0 1 9 2 0 4 5 3 5 6 8 5 7\n",
      " 2 5 0 3 0 3 8 5 8 9 1 3 2 4 2 6 5 6 3 3 9 7 8 8 7 7 2 5]\n",
      "[31/100][199/200] Loss_D: -0.1063 (0.0812) Loss_G: 0.0048 (0.2896) D(x): 0.5113 D(G(z)): 0.4137 / 0.4067 Acc: 44.0000 (35.5249)\n",
      "[32/100][199/200] Loss_D: -0.1257 (0.0759) Loss_G: -0.2640 (0.2790) D(x): 0.5101 D(G(z)): 0.4207 / 0.5177 Acc: 46.4000 (35.7912)\n",
      "[33/100][199/200] Loss_D: -0.0384 (0.0707) Loss_G: -0.1813 (0.2690) D(x): 0.5609 D(G(z)): 0.5160 / 0.4742 Acc: 44.4000 (36.0441)\n",
      "[34/100][199/200] Loss_D: -0.2729 (0.0659) Loss_G: -0.0212 (0.2595) D(x): 0.5611 D(G(z)): 0.4084 / 0.4056 Acc: 50.0000 (36.3009)\n",
      "[35/100][199/200] Loss_D: 0.0454 (0.0613) Loss_G: -0.3098 (0.2507) D(x): 0.5575 D(G(z)): 0.5234 / 0.5372 Acc: 42.0000 (36.5467)\n",
      "Label for eval = [7 9 7 2 6 4 9 4 7 4 4 1 5 4 6 5 8 1 6 4 2 8 6 5 8 0 8 4 3 9 0 0 3 5 7 6 1\n",
      " 6 7 2 7 9 6 4 4 1 0 1 3 7 4 1 0 5 7 7 9 5 9 0 7 4 9 5 1 7 8 7 4 4 6 3 2 7\n",
      " 2 1 8 8 1 1 2 8 7 3 8 9 9 8 5 4 4 6 4 2 8 2 1 6 1 5 3 1 2 2 9 9 7 4 4 0 4\n",
      " 6 2 7 9 8 0 2 4 8 4 8 5 8 6 9 4 9 1 6 3 7 6 8 5 2 7 4 0 5 7 2 6 3 5 4 9 0\n",
      " 6 9 9 0 8 3 1 3 8 4 3 2 4 8 4 0 0 2 8 7 3 1 9 4 4 4 2 2 9 2 4 1 9 7 4 2 6\n",
      " 8 6 4 5 0 1 4 9 7 1 5 2 5 4 7 5 6 8 6 5 9 3 9 6 0 1 9 2 0 4 5 3 5 6 8 5 7\n",
      " 2 5 0 3 0 3 8 5 8 9 1 3 2 4 2 6 5 6 3 3 9 7 8 8 7 7 2 5]\n",
      "[36/100][199/200] Loss_D: -0.0742 (0.0567) Loss_G: -0.0384 (0.2430) D(x): 0.5778 D(G(z)): 0.5105 / 0.4272 Acc: 47.2000 (36.7883)\n",
      "[37/100][199/200] Loss_D: -0.0527 (0.0521) Loss_G: -0.2162 (0.2354) D(x): 0.4470 D(G(z)): 0.3628 / 0.5032 Acc: 44.8000 (37.0156)\n",
      "[38/100][199/200] Loss_D: -0.1007 (0.0480) Loss_G: -0.0123 (0.2282) D(x): 0.6448 D(G(z)): 0.5482 / 0.4091 Acc: 46.0000 (37.2409)\n",
      "[39/100][199/200] Loss_D: -0.0567 (0.0443) Loss_G: -0.0312 (0.2218) D(x): 0.6498 D(G(z)): 0.5719 / 0.4222 Acc: 45.2000 (37.4621)\n",
      "[40/100][199/200] Loss_D: -0.1088 (0.0407) Loss_G: 0.0556 (0.2155) D(x): 0.6019 D(G(z)): 0.4959 / 0.3781 Acc: 41.6000 (37.6747)\n",
      "Label for eval = [7 9 7 2 6 4 9 4 7 4 4 1 5 4 6 5 8 1 6 4 2 8 6 5 8 0 8 4 3 9 0 0 3 5 7 6 1\n",
      " 6 7 2 7 9 6 4 4 1 0 1 3 7 4 1 0 5 7 7 9 5 9 0 7 4 9 5 1 7 8 7 4 4 6 3 2 7\n",
      " 2 1 8 8 1 1 2 8 7 3 8 9 9 8 5 4 4 6 4 2 8 2 1 6 1 5 3 1 2 2 9 9 7 4 4 0 4\n",
      " 6 2 7 9 8 0 2 4 8 4 8 5 8 6 9 4 9 1 6 3 7 6 8 5 2 7 4 0 5 7 2 6 3 5 4 9 0\n",
      " 6 9 9 0 8 3 1 3 8 4 3 2 4 8 4 0 0 2 8 7 3 1 9 4 4 4 2 2 9 2 4 1 9 7 4 2 6\n",
      " 8 6 4 5 0 1 4 9 7 1 5 2 5 4 7 5 6 8 6 5 9 3 9 6 0 1 9 2 0 4 5 3 5 6 8 5 7\n",
      " 2 5 0 3 0 3 8 5 8 9 1 3 2 4 2 6 5 6 3 3 9 7 8 8 7 7 2 5]\n",
      "[41/100][199/200] Loss_D: -0.2264 (0.0370) Loss_G: -0.1261 (0.2090) D(x): 0.5137 D(G(z)): 0.3658 / 0.4573 Acc: 44.4000 (37.8859)\n",
      "[42/100][199/200] Loss_D: 0.1699 (0.0332) Loss_G: -0.1330 (0.2027) D(x): 0.3661 D(G(z)): 0.3865 / 0.4511 Acc: 45.2000 (38.0958)\n",
      "[43/100][199/200] Loss_D: -0.1891 (0.0297) Loss_G: -0.0193 (0.1969) D(x): 0.5807 D(G(z)): 0.4563 / 0.4150 Acc: 45.6000 (38.3029)\n",
      "[44/100][199/200] Loss_D: -0.1521 (0.0264) Loss_G: -0.0632 (0.1916) D(x): 0.5560 D(G(z)): 0.4461 / 0.4354 Acc: 47.2000 (38.5078)\n",
      "[45/100][199/200] Loss_D: -0.3258 (0.0234) Loss_G: 0.0641 (0.1864) D(x): 0.6355 D(G(z)): 0.4361 / 0.3824 Acc: 44.4000 (38.7066)\n",
      "Label for eval = [7 9 7 2 6 4 9 4 7 4 4 1 5 4 6 5 8 1 6 4 2 8 6 5 8 0 8 4 3 9 0 0 3 5 7 6 1\n",
      " 6 7 2 7 9 6 4 4 1 0 1 3 7 4 1 0 5 7 7 9 5 9 0 7 4 9 5 1 7 8 7 4 4 6 3 2 7\n",
      " 2 1 8 8 1 1 2 8 7 3 8 9 9 8 5 4 4 6 4 2 8 2 1 6 1 5 3 1 2 2 9 9 7 4 4 0 4\n",
      " 6 2 7 9 8 0 2 4 8 4 8 5 8 6 9 4 9 1 6 3 7 6 8 5 2 7 4 0 5 7 2 6 3 5 4 9 0\n",
      " 6 9 9 0 8 3 1 3 8 4 3 2 4 8 4 0 0 2 8 7 3 1 9 4 4 4 2 2 9 2 4 1 9 7 4 2 6\n",
      " 8 6 4 5 0 1 4 9 7 1 5 2 5 4 7 5 6 8 6 5 9 3 9 6 0 1 9 2 0 4 5 3 5 6 8 5 7\n",
      " 2 5 0 3 0 3 8 5 8 9 1 3 2 4 2 6 5 6 3 3 9 7 8 8 7 7 2 5]\n",
      "[46/100][199/200] Loss_D: -0.1769 (0.0203) Loss_G: -0.1183 (0.1815) D(x): 0.5262 D(G(z)): 0.4082 / 0.4619 Acc: 44.0000 (38.8917)\n",
      "[47/100][199/200] Loss_D: -0.1910 (0.0170) Loss_G: 0.0974 (0.1771) D(x): 0.5419 D(G(z)): 0.4567 / 0.3660 Acc: 51.6000 (39.0792)\n",
      "[48/100][199/200] Loss_D: -0.0733 (0.0140) Loss_G: 0.1819 (0.1727) D(x): 0.5963 D(G(z)): 0.5314 / 0.3474 Acc: 46.8000 (39.2606)\n",
      "[49/100][199/200] Loss_D: -0.1080 (0.0109) Loss_G: 0.1134 (0.1687) D(x): 0.6221 D(G(z)): 0.5425 / 0.3685 Acc: 49.2000 (39.4436)\n",
      "[50/100][199/200] Loss_D: -0.0976 (0.0079) Loss_G: -0.0607 (0.1646) D(x): 0.6339 D(G(z)): 0.5587 / 0.4315 Acc: 48.8000 (39.6164)\n",
      "Label for eval = [7 9 7 2 6 4 9 4 7 4 4 1 5 4 6 5 8 1 6 4 2 8 6 5 8 0 8 4 3 9 0 0 3 5 7 6 1\n",
      " 6 7 2 7 9 6 4 4 1 0 1 3 7 4 1 0 5 7 7 9 5 9 0 7 4 9 5 1 7 8 7 4 4 6 3 2 7\n",
      " 2 1 8 8 1 1 2 8 7 3 8 9 9 8 5 4 4 6 4 2 8 2 1 6 1 5 3 1 2 2 9 9 7 4 4 0 4\n",
      " 6 2 7 9 8 0 2 4 8 4 8 5 8 6 9 4 9 1 6 3 7 6 8 5 2 7 4 0 5 7 2 6 3 5 4 9 0\n",
      " 6 9 9 0 8 3 1 3 8 4 3 2 4 8 4 0 0 2 8 7 3 1 9 4 4 4 2 2 9 2 4 1 9 7 4 2 6\n",
      " 8 6 4 5 0 1 4 9 7 1 5 2 5 4 7 5 6 8 6 5 9 3 9 6 0 1 9 2 0 4 5 3 5 6 8 5 7\n",
      " 2 5 0 3 0 3 8 5 8 9 1 3 2 4 2 6 5 6 3 3 9 7 8 8 7 7 2 5]\n",
      "[51/100][199/200] Loss_D: -0.3034 (0.0052) Loss_G: -0.0606 (0.1607) D(x): 0.6123 D(G(z)): 0.4581 / 0.4262 Acc: 53.2000 (39.7877)\n",
      "[52/100][199/200] Loss_D: -0.1025 (0.0022) Loss_G: 0.1366 (0.1572) D(x): 0.5119 D(G(z)): 0.4353 / 0.3599 Acc: 46.0000 (39.9680)\n",
      "[53/100][199/200] Loss_D: -0.1109 (-0.0007) Loss_G: -0.2692 (0.1538) D(x): 0.4615 D(G(z)): 0.3917 / 0.5207 Acc: 52.0000 (40.1336)\n",
      "[54/100][199/200] Loss_D: -0.0428 (-0.0035) Loss_G: -0.3387 (0.1508) D(x): 0.3830 D(G(z)): 0.3154 / 0.5613 Acc: 49.2000 (40.2951)\n",
      "[55/100][199/200] Loss_D: -0.2648 (-0.0063) Loss_G: 0.0863 (0.1480) D(x): 0.5433 D(G(z)): 0.3982 / 0.3779 Acc: 51.6000 (40.4557)\n",
      "Label for eval = [7 9 7 2 6 4 9 4 7 4 4 1 5 4 6 5 8 1 6 4 2 8 6 5 8 0 8 4 3 9 0 0 3 5 7 6 1\n",
      " 6 7 2 7 9 6 4 4 1 0 1 3 7 4 1 0 5 7 7 9 5 9 0 7 4 9 5 1 7 8 7 4 4 6 3 2 7\n",
      " 2 1 8 8 1 1 2 8 7 3 8 9 9 8 5 4 4 6 4 2 8 2 1 6 1 5 3 1 2 2 9 9 7 4 4 0 4\n",
      " 6 2 7 9 8 0 2 4 8 4 8 5 8 6 9 4 9 1 6 3 7 6 8 5 2 7 4 0 5 7 2 6 3 5 4 9 0\n",
      " 6 9 9 0 8 3 1 3 8 4 3 2 4 8 4 0 0 2 8 7 3 1 9 4 4 4 2 2 9 2 4 1 9 7 4 2 6\n",
      " 8 6 4 5 0 1 4 9 7 1 5 2 5 4 7 5 6 8 6 5 9 3 9 6 0 1 9 2 0 4 5 3 5 6 8 5 7\n",
      " 2 5 0 3 0 3 8 5 8 9 1 3 2 4 2 6 5 6 3 3 9 7 8 8 7 7 2 5]\n",
      "[56/100][199/200] Loss_D: -0.0950 (-0.0091) Loss_G: -0.2408 (0.1455) D(x): 0.4585 D(G(z)): 0.3799 / 0.5092 Acc: 48.4000 (40.6094)\n",
      "[57/100][199/200] Loss_D: -0.2187 (-0.0116) Loss_G: -0.0609 (0.1430) D(x): 0.5047 D(G(z)): 0.3970 / 0.4313 Acc: 53.6000 (40.7660)\n",
      "[58/100][199/200] Loss_D: -0.1060 (-0.0145) Loss_G: 0.3124 (0.1404) D(x): 0.6766 D(G(z)): 0.5757 / 0.3045 Acc: 46.4000 (40.9185)\n",
      "[59/100][199/200] Loss_D: 0.0049 (-0.0171) Loss_G: 0.1994 (0.1384) D(x): 0.6789 D(G(z)): 0.6235 / 0.3475 Acc: 47.6000 (41.0656)\n",
      "[60/100][199/200] Loss_D: -0.3197 (-0.0198) Loss_G: 0.0485 (0.1367) D(x): 0.5523 D(G(z)): 0.3896 / 0.3938 Acc: 54.4000 (41.2106)\n",
      "Label for eval = [7 9 7 2 6 4 9 4 7 4 4 1 5 4 6 5 8 1 6 4 2 8 6 5 8 0 8 4 3 9 0 0 3 5 7 6 1\n",
      " 6 7 2 7 9 6 4 4 1 0 1 3 7 4 1 0 5 7 7 9 5 9 0 7 4 9 5 1 7 8 7 4 4 6 3 2 7\n",
      " 2 1 8 8 1 1 2 8 7 3 8 9 9 8 5 4 4 6 4 2 8 2 1 6 1 5 3 1 2 2 9 9 7 4 4 0 4\n",
      " 6 2 7 9 8 0 2 4 8 4 8 5 8 6 9 4 9 1 6 3 7 6 8 5 2 7 4 0 5 7 2 6 3 5 4 9 0\n",
      " 6 9 9 0 8 3 1 3 8 4 3 2 4 8 4 0 0 2 8 7 3 1 9 4 4 4 2 2 9 2 4 1 9 7 4 2 6\n",
      " 8 6 4 5 0 1 4 9 7 1 5 2 5 4 7 5 6 8 6 5 9 3 9 6 0 1 9 2 0 4 5 3 5 6 8 5 7\n",
      " 2 5 0 3 0 3 8 5 8 9 1 3 2 4 2 6 5 6 3 3 9 7 8 8 7 7 2 5]\n",
      "[61/100][199/200] Loss_D: -0.1367 (-0.0224) Loss_G: 0.3116 (0.1349) D(x): 0.5730 D(G(z)): 0.4776 / 0.3149 Acc: 48.0000 (41.3565)\n",
      "[62/100][199/200] Loss_D: -0.2390 (-0.0253) Loss_G: 0.1484 (0.1336) D(x): 0.6038 D(G(z)): 0.4645 / 0.3661 Acc: 48.8000 (41.4995)\n",
      "[63/100][199/200] Loss_D: -0.2261 (-0.0278) Loss_G: 0.1025 (0.1321) D(x): 0.5176 D(G(z)): 0.4066 / 0.3753 Acc: 54.4000 (41.6357)\n",
      "[64/100][199/200] Loss_D: -0.3049 (-0.0303) Loss_G: 0.2929 (0.1310) D(x): 0.6721 D(G(z)): 0.4746 / 0.3216 Acc: 48.0000 (41.7671)\n",
      "[65/100][199/200] Loss_D: -0.2937 (-0.0326) Loss_G: 0.0667 (0.1298) D(x): 0.6030 D(G(z)): 0.4436 / 0.3872 Acc: 50.8000 (41.8964)\n",
      "Label for eval = [7 9 7 2 6 4 9 4 7 4 4 1 5 4 6 5 8 1 6 4 2 8 6 5 8 0 8 4 3 9 0 0 3 5 7 6 1\n",
      " 6 7 2 7 9 6 4 4 1 0 1 3 7 4 1 0 5 7 7 9 5 9 0 7 4 9 5 1 7 8 7 4 4 6 3 2 7\n",
      " 2 1 8 8 1 1 2 8 7 3 8 9 9 8 5 4 4 6 4 2 8 2 1 6 1 5 3 1 2 2 9 9 7 4 4 0 4\n",
      " 6 2 7 9 8 0 2 4 8 4 8 5 8 6 9 4 9 1 6 3 7 6 8 5 2 7 4 0 5 7 2 6 3 5 4 9 0\n",
      " 6 9 9 0 8 3 1 3 8 4 3 2 4 8 4 0 0 2 8 7 3 1 9 4 4 4 2 2 9 2 4 1 9 7 4 2 6\n",
      " 8 6 4 5 0 1 4 9 7 1 5 2 5 4 7 5 6 8 6 5 9 3 9 6 0 1 9 2 0 4 5 3 5 6 8 5 7\n",
      " 2 5 0 3 0 3 8 5 8 9 1 3 2 4 2 6 5 6 3 3 9 7 8 8 7 7 2 5]\n",
      "[66/100][199/200] Loss_D: -0.4938 (-0.0349) Loss_G: 0.2184 (0.1285) D(x): 0.6561 D(G(z)): 0.4243 / 0.3357 Acc: 55.2000 (42.0219)\n",
      "[67/100][199/200] Loss_D: -0.3085 (-0.0374) Loss_G: 0.1015 (0.1275) D(x): 0.6005 D(G(z)): 0.4137 / 0.3863 Acc: 50.0000 (42.1469)\n",
      "[68/100][199/200] Loss_D: -0.0601 (-0.0398) Loss_G: -0.0903 (0.1270) D(x): 0.4603 D(G(z)): 0.4034 / 0.4625 Acc: 55.2000 (42.2742)\n",
      "[69/100][199/200] Loss_D: -0.1976 (-0.0421) Loss_G: 0.3595 (0.1260) D(x): 0.6673 D(G(z)): 0.5351 / 0.3136 Acc: 50.8000 (42.3925)\n",
      "[70/100][199/200] Loss_D: -0.3578 (-0.0445) Loss_G: 0.1850 (0.1252) D(x): 0.6328 D(G(z)): 0.4284 / 0.3546 Acc: 49.2000 (42.5129)\n",
      "Label for eval = [7 9 7 2 6 4 9 4 7 4 4 1 5 4 6 5 8 1 6 4 2 8 6 5 8 0 8 4 3 9 0 0 3 5 7 6 1\n",
      " 6 7 2 7 9 6 4 4 1 0 1 3 7 4 1 0 5 7 7 9 5 9 0 7 4 9 5 1 7 8 7 4 4 6 3 2 7\n",
      " 2 1 8 8 1 1 2 8 7 3 8 9 9 8 5 4 4 6 4 2 8 2 1 6 1 5 3 1 2 2 9 9 7 4 4 0 4\n",
      " 6 2 7 9 8 0 2 4 8 4 8 5 8 6 9 4 9 1 6 3 7 6 8 5 2 7 4 0 5 7 2 6 3 5 4 9 0\n",
      " 6 9 9 0 8 3 1 3 8 4 3 2 4 8 4 0 0 2 8 7 3 1 9 4 4 4 2 2 9 2 4 1 9 7 4 2 6\n",
      " 8 6 4 5 0 1 4 9 7 1 5 2 5 4 7 5 6 8 6 5 9 3 9 6 0 1 9 2 0 4 5 3 5 6 8 5 7\n",
      " 2 5 0 3 0 3 8 5 8 9 1 3 2 4 2 6 5 6 3 3 9 7 8 8 7 7 2 5]\n",
      "[71/100][199/200] Loss_D: -0.0085 (-0.0468) Loss_G: -0.0259 (0.1243) D(x): 0.5286 D(G(z)): 0.4919 / 0.4326 Acc: 49.6000 (42.6255)\n",
      "[72/100][199/200] Loss_D: -0.3178 (-0.0491) Loss_G: 0.0838 (0.1239) D(x): 0.4633 D(G(z)): 0.2932 / 0.3794 Acc: 55.2000 (42.7365)\n",
      "[73/100][199/200] Loss_D: -0.1119 (-0.0513) Loss_G: 0.2693 (0.1234) D(x): 0.6023 D(G(z)): 0.5301 / 0.3300 Acc: 52.0000 (42.8472)\n",
      "[74/100][199/200] Loss_D: -0.1019 (-0.0539) Loss_G: -0.0318 (0.1231) D(x): 0.6542 D(G(z)): 0.5600 / 0.4427 Acc: 52.8000 (42.9547)\n",
      "[75/100][199/200] Loss_D: -0.2957 (-0.0561) Loss_G: 0.4724 (0.1231) D(x): 0.5222 D(G(z)): 0.3612 / 0.2748 Acc: 52.8000 (43.0547)\n",
      "Label for eval = [7 9 7 2 6 4 9 4 7 4 4 1 5 4 6 5 8 1 6 4 2 8 6 5 8 0 8 4 3 9 0 0 3 5 7 6 1\n",
      " 6 7 2 7 9 6 4 4 1 0 1 3 7 4 1 0 5 7 7 9 5 9 0 7 4 9 5 1 7 8 7 4 4 6 3 2 7\n",
      " 2 1 8 8 1 1 2 8 7 3 8 9 9 8 5 4 4 6 4 2 8 2 1 6 1 5 3 1 2 2 9 9 7 4 4 0 4\n",
      " 6 2 7 9 8 0 2 4 8 4 8 5 8 6 9 4 9 1 6 3 7 6 8 5 2 7 4 0 5 7 2 6 3 5 4 9 0\n",
      " 6 9 9 0 8 3 1 3 8 4 3 2 4 8 4 0 0 2 8 7 3 1 9 4 4 4 2 2 9 2 4 1 9 7 4 2 6\n",
      " 8 6 4 5 0 1 4 9 7 1 5 2 5 4 7 5 6 8 6 5 9 3 9 6 0 1 9 2 0 4 5 3 5 6 8 5 7\n",
      " 2 5 0 3 0 3 8 5 8 9 1 3 2 4 2 6 5 6 3 3 9 7 8 8 7 7 2 5]\n",
      "[76/100][199/200] Loss_D: -0.0483 (-0.0583) Loss_G: 0.9664 (0.1231) D(x): 0.6297 D(G(z)): 0.5589 / 0.1739 Acc: 47.2000 (43.1604)\n",
      "[77/100][199/200] Loss_D: -0.4099 (-0.0605) Loss_G: 0.2375 (0.1225) D(x): 0.6874 D(G(z)): 0.4385 / 0.3324 Acc: 48.4000 (43.2607)\n",
      "[78/100][199/200] Loss_D: -0.2472 (-0.0628) Loss_G: -0.0198 (0.1228) D(x): 0.5454 D(G(z)): 0.3704 / 0.4251 Acc: 49.2000 (43.3597)\n",
      "[79/100][199/200] Loss_D: -0.2538 (-0.0649) Loss_G: 0.2240 (0.1233) D(x): 0.4924 D(G(z)): 0.3383 / 0.3459 Acc: 50.0000 (43.4564)\n",
      "[80/100][199/200] Loss_D: -0.1059 (-0.0673) Loss_G: -0.3784 (0.1234) D(x): 0.4185 D(G(z)): 0.3348 / 0.5882 Acc: 54.8000 (43.5529)\n",
      "Label for eval = [7 9 7 2 6 4 9 4 7 4 4 1 5 4 6 5 8 1 6 4 2 8 6 5 8 0 8 4 3 9 0 0 3 5 7 6 1\n",
      " 6 7 2 7 9 6 4 4 1 0 1 3 7 4 1 0 5 7 7 9 5 9 0 7 4 9 5 1 7 8 7 4 4 6 3 2 7\n",
      " 2 1 8 8 1 1 2 8 7 3 8 9 9 8 5 4 4 6 4 2 8 2 1 6 1 5 3 1 2 2 9 9 7 4 4 0 4\n",
      " 6 2 7 9 8 0 2 4 8 4 8 5 8 6 9 4 9 1 6 3 7 6 8 5 2 7 4 0 5 7 2 6 3 5 4 9 0\n",
      " 6 9 9 0 8 3 1 3 8 4 3 2 4 8 4 0 0 2 8 7 3 1 9 4 4 4 2 2 9 2 4 1 9 7 4 2 6\n",
      " 8 6 4 5 0 1 4 9 7 1 5 2 5 4 7 5 6 8 6 5 9 3 9 6 0 1 9 2 0 4 5 3 5 6 8 5 7\n",
      " 2 5 0 3 0 3 8 5 8 9 1 3 2 4 2 6 5 6 3 3 9 7 8 8 7 7 2 5]\n",
      "[81/100][199/200] Loss_D: -0.3990 (-0.0693) Loss_G: -0.1008 (0.1235) D(x): 0.6721 D(G(z)): 0.4581 / 0.4602 Acc: 53.2000 (43.6506)\n",
      "[82/100][199/200] Loss_D: -0.3664 (-0.0715) Loss_G: 0.4470 (0.1237) D(x): 0.6516 D(G(z)): 0.4361 / 0.2883 Acc: 48.8000 (43.7421)\n",
      "[83/100][199/200] Loss_D: -0.2632 (-0.0738) Loss_G: 0.0392 (0.1242) D(x): 0.5434 D(G(z)): 0.3681 / 0.4248 Acc: 48.8000 (43.8323)\n",
      "[84/100][199/200] Loss_D: -0.4482 (-0.0758) Loss_G: 0.2146 (0.1250) D(x): 0.6659 D(G(z)): 0.3787 / 0.3588 Acc: 45.6000 (43.9177)\n",
      "[85/100][199/200] Loss_D: -0.2058 (-0.0779) Loss_G: 0.7021 (0.1254) D(x): 0.6577 D(G(z)): 0.5357 / 0.2351 Acc: 54.8000 (44.0030)\n",
      "Label for eval = [7 9 7 2 6 4 9 4 7 4 4 1 5 4 6 5 8 1 6 4 2 8 6 5 8 0 8 4 3 9 0 0 3 5 7 6 1\n",
      " 6 7 2 7 9 6 4 4 1 0 1 3 7 4 1 0 5 7 7 9 5 9 0 7 4 9 5 1 7 8 7 4 4 6 3 2 7\n",
      " 2 1 8 8 1 1 2 8 7 3 8 9 9 8 5 4 4 6 4 2 8 2 1 6 1 5 3 1 2 2 9 9 7 4 4 0 4\n",
      " 6 2 7 9 8 0 2 4 8 4 8 5 8 6 9 4 9 1 6 3 7 6 8 5 2 7 4 0 5 7 2 6 3 5 4 9 0\n",
      " 6 9 9 0 8 3 1 3 8 4 3 2 4 8 4 0 0 2 8 7 3 1 9 4 4 4 2 2 9 2 4 1 9 7 4 2 6\n",
      " 8 6 4 5 0 1 4 9 7 1 5 2 5 4 7 5 6 8 6 5 9 3 9 6 0 1 9 2 0 4 5 3 5 6 8 5 7\n",
      " 2 5 0 3 0 3 8 5 8 9 1 3 2 4 2 6 5 6 3 3 9 7 8 8 7 7 2 5]\n",
      "[86/100][199/200] Loss_D: -0.3526 (-0.0798) Loss_G: 0.3597 (0.1255) D(x): 0.5143 D(G(z)): 0.2862 / 0.3072 Acc: 50.8000 (44.0865)\n",
      "[87/100][199/200] Loss_D: -0.3492 (-0.0820) Loss_G: 0.6816 (0.1259) D(x): 0.7086 D(G(z)): 0.4539 / 0.2325 Acc: 43.2000 (44.1698)\n",
      "[88/100][199/200] Loss_D: -0.0231 (-0.0841) Loss_G: 0.3664 (0.1260) D(x): 0.3758 D(G(z)): 0.2266 / 0.3119 Acc: 44.0000 (44.2527)\n",
      "[89/100][199/200] Loss_D: -0.3188 (-0.0862) Loss_G: -0.0872 (0.1267) D(x): 0.6078 D(G(z)): 0.4223 / 0.4546 Acc: 51.6000 (44.3327)\n",
      "[90/100][199/200] Loss_D: -0.5799 (-0.0880) Loss_G: 0.4753 (0.1270) D(x): 0.6713 D(G(z)): 0.3493 / 0.2844 Acc: 52.0000 (44.4112)\n",
      "Label for eval = [7 9 7 2 6 4 9 4 7 4 4 1 5 4 6 5 8 1 6 4 2 8 6 5 8 0 8 4 3 9 0 0 3 5 7 6 1\n",
      " 6 7 2 7 9 6 4 4 1 0 1 3 7 4 1 0 5 7 7 9 5 9 0 7 4 9 5 1 7 8 7 4 4 6 3 2 7\n",
      " 2 1 8 8 1 1 2 8 7 3 8 9 9 8 5 4 4 6 4 2 8 2 1 6 1 5 3 1 2 2 9 9 7 4 4 0 4\n",
      " 6 2 7 9 8 0 2 4 8 4 8 5 8 6 9 4 9 1 6 3 7 6 8 5 2 7 4 0 5 7 2 6 3 5 4 9 0\n",
      " 6 9 9 0 8 3 1 3 8 4 3 2 4 8 4 0 0 2 8 7 3 1 9 4 4 4 2 2 9 2 4 1 9 7 4 2 6\n",
      " 8 6 4 5 0 1 4 9 7 1 5 2 5 4 7 5 6 8 6 5 9 3 9 6 0 1 9 2 0 4 5 3 5 6 8 5 7\n",
      " 2 5 0 3 0 3 8 5 8 9 1 3 2 4 2 6 5 6 3 3 9 7 8 8 7 7 2 5]\n",
      "[91/100][199/200] Loss_D: -0.2153 (-0.0899) Loss_G: 0.5661 (0.1276) D(x): 0.6196 D(G(z)): 0.5097 / 0.2623 Acc: 57.6000 (44.4915)\n",
      "[92/100][199/200] Loss_D: -0.2353 (-0.0918) Loss_G: 0.5547 (0.1281) D(x): 0.6664 D(G(z)): 0.5227 / 0.2649 Acc: 52.0000 (44.5722)\n",
      "[93/100][199/200] Loss_D: -0.2391 (-0.0936) Loss_G: -0.2296 (0.1287) D(x): 0.4879 D(G(z)): 0.3354 / 0.5257 Acc: 54.0000 (44.6493)\n",
      "[94/100][199/200] Loss_D: -0.0947 (-0.0953) Loss_G: 0.0325 (0.1291) D(x): 0.5344 D(G(z)): 0.4893 / 0.4166 Acc: 55.6000 (44.7242)\n",
      "[95/100][199/200] Loss_D: -0.5491 (-0.0971) Loss_G: 0.5255 (0.1294) D(x): 0.6410 D(G(z)): 0.3360 / 0.2686 Acc: 56.4000 (44.8007)\n",
      "Label for eval = [7 9 7 2 6 4 9 4 7 4 4 1 5 4 6 5 8 1 6 4 2 8 6 5 8 0 8 4 3 9 0 0 3 5 7 6 1\n",
      " 6 7 2 7 9 6 4 4 1 0 1 3 7 4 1 0 5 7 7 9 5 9 0 7 4 9 5 1 7 8 7 4 4 6 3 2 7\n",
      " 2 1 8 8 1 1 2 8 7 3 8 9 9 8 5 4 4 6 4 2 8 2 1 6 1 5 3 1 2 2 9 9 7 4 4 0 4\n",
      " 6 2 7 9 8 0 2 4 8 4 8 5 8 6 9 4 9 1 6 3 7 6 8 5 2 7 4 0 5 7 2 6 3 5 4 9 0\n",
      " 6 9 9 0 8 3 1 3 8 4 3 2 4 8 4 0 0 2 8 7 3 1 9 4 4 4 2 2 9 2 4 1 9 7 4 2 6\n",
      " 8 6 4 5 0 1 4 9 7 1 5 2 5 4 7 5 6 8 6 5 9 3 9 6 0 1 9 2 0 4 5 3 5 6 8 5 7\n",
      " 2 5 0 3 0 3 8 5 8 9 1 3 2 4 2 6 5 6 3 3 9 7 8 8 7 7 2 5]\n",
      "[96/100][199/200] Loss_D: -0.2809 (-0.0985) Loss_G: 0.0932 (0.1299) D(x): 0.6881 D(G(z)): 0.5065 / 0.3857 Acc: 54.4000 (44.8710)\n",
      "[97/100][199/200] Loss_D: -0.1418 (-0.1004) Loss_G: 0.4764 (0.1304) D(x): 0.6312 D(G(z)): 0.5053 / 0.2854 Acc: 48.4000 (44.9413)\n",
      "[98/100][199/200] Loss_D: 0.0001 (-0.1021) Loss_G: -0.0866 (0.1308) D(x): 0.4753 D(G(z)): 0.4483 / 0.4639 Acc: 52.8000 (45.0091)\n",
      "[99/100][199/200] Loss_D: -0.1769 (-0.1038) Loss_G: 0.0724 (0.1314) D(x): 0.4104 D(G(z)): 0.2410 / 0.4046 Acc: 54.4000 (45.0825)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "nz = 110\n",
    "num_classes = 10\n",
    "d_lr = 0.0001\n",
    "g_lr = 0.0002\n",
    "epochs = 100\n",
    "\n",
    "# datase t\n",
    "dataset = dset.CIFAR10(\n",
    "    root='./data', download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ]))\n",
    "\n",
    "\n",
    "batch_size = 250\n",
    "ngpu = 1\n",
    "\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=int(0))\n",
    "\n",
    "# some hyper parameters\n",
    "\n",
    "\n",
    "# Define the generator and initialize the weights\n",
    "netG = _netG_CIFAR10(ngpu, nz)\n",
    "netG.apply(weights_init)\n",
    "\n",
    "# Define the discriminator and initialize the weights\n",
    "netD = _netD_CIFAR10(ngpu, num_classes)\n",
    "netD.apply(weights_init)\n",
    "\n",
    "# loss functions\n",
    "dis_criterion = nn.BCELoss()\n",
    "aux_criterion = nn.NLLLoss()\n",
    "\n",
    "\n",
    "im_size = 32\n",
    "\n",
    "# tensor placeholders\n",
    "input = torch.FloatTensor(batch_size, 3,im_size, im_size)\n",
    "noise = torch.FloatTensor(batch_size, nz, 1, 1)\n",
    "eval_noise = torch.FloatTensor(batch_size, nz, 1, 1).normal_(0, 1)\n",
    "dis_label = torch.FloatTensor(batch_size)\n",
    "aux_label = torch.LongTensor(batch_size)\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# if using cuda\n",
    "netD.cuda()\n",
    "netG.cuda()\n",
    "dis_criterion.cuda()\n",
    "aux_criterion.cuda()\n",
    "input, dis_label, aux_label = input.cuda(), dis_label.cuda(), aux_label.cuda()\n",
    "noise, eval_noise = noise.cuda(), eval_noise.cuda()\n",
    "\n",
    "# define variables\n",
    "input = Variable(input)\n",
    "noise = Variable(noise)\n",
    "eval_noise = Variable(eval_noise)\n",
    "dis_label = Variable(dis_label)\n",
    "aux_label = Variable(aux_label)\n",
    "# noise for evaluation\n",
    "eval_noise_ = np.random.normal(0, 1, (batch_size, nz))\n",
    "eval_label = np.random.randint(0, num_classes, batch_size)\n",
    "eval_onehot = np.zeros((batch_size, num_classes))\n",
    "eval_onehot[np.arange(batch_size), eval_label] = 1\n",
    "eval_noise_[np.arange(batch_size), :num_classes] = eval_onehot[np.arange(batch_size)]\n",
    "eval_noise_ = (torch.from_numpy(eval_noise_))\n",
    "eval_noise.data.copy_(eval_noise_.view(batch_size, nz, 1, 1))\n",
    "\n",
    "# setup optimizer\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "avg_loss_D = 0.0\n",
    "avg_loss_G = 0.0\n",
    "avg_loss_A = 0.0\n",
    "def compute_acc(preds, labels):\n",
    "    correct = 0\n",
    "    preds_ = preds.data.max(1)[1]\n",
    "    correct = preds_.eq(labels.data).cpu().sum()\n",
    "    acc = float(correct) / float(len(labels.data)) * 100.0\n",
    "    return acc\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        # train with real\n",
    "        netD.zero_grad()\n",
    "        real_cpu, label = data\n",
    "        batch_size = real_cpu.size(0)\n",
    "        real_cpu = real_cpu.cuda()\n",
    "        input.data.resize_as_(real_cpu).copy_(real_cpu)\n",
    "        dis_label.data.resize_(batch_size).fill_(real_label)\n",
    "        aux_label.data.resize_(batch_size).copy_(label)\n",
    "        dis_output, aux_output = netD(input)\n",
    "\n",
    "        dis_errD_real = dis_criterion(dis_output, dis_label)\n",
    "        aux_errD_real = aux_criterion(aux_output, aux_label)\n",
    "        errD_real = dis_errD_real + aux_errD_real\n",
    "        errD_real.backward()\n",
    "        D_x = dis_output.data.mean()\n",
    "\n",
    "        # compute the current classification accuracy\n",
    "        accuracy = compute_acc(aux_output, aux_label)\n",
    "\n",
    "        # train with fake\n",
    "        noise.data.resize_(batch_size, nz, 1, 1).normal_(0, 1)\n",
    "        label = np.random.randint(0, num_classes, batch_size)\n",
    "        noise_ = np.random.normal(0, 1, (batch_size, nz))\n",
    "        class_onehot = np.zeros((batch_size, num_classes))\n",
    "        class_onehot[np.arange(batch_size), label] = 1\n",
    "        noise_[np.arange(batch_size), :num_classes] = class_onehot[np.arange(batch_size)]\n",
    "        noise_ = (torch.from_numpy(noise_))\n",
    "        noise.data.copy_(noise_.view(batch_size, nz, 1, 1))\n",
    "        aux_label.data.resize_(batch_size).copy_(torch.from_numpy(label))\n",
    "\n",
    "        fake = netG(noise)\n",
    "        dis_label.data.fill_(fake_label)\n",
    "        dis_output, aux_output = netD(fake.detach())\n",
    "        dis_errD_fake = dis_criterion(dis_output, dis_label)\n",
    "        aux_errD_fake = aux_criterion(aux_output, aux_label)\n",
    "        errD_fake = dis_errD_fake + aux_errD_fake\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = dis_output.data.mean()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        dis_label.data.fill_(real_label)  # fake labels are real for generator cost\n",
    "        dis_output, aux_output = netD(fake)\n",
    "        dis_errG = dis_criterion(dis_output, dis_label)\n",
    "        aux_errG = aux_criterion(aux_output, aux_label)\n",
    "        errG = dis_errG + aux_errG\n",
    "        errG.backward()\n",
    "        D_G_z2 = dis_output.data.mean()\n",
    "        optimizerG.step()\n",
    "\n",
    "        # compute the average loss\n",
    "        curr_iter = epoch * len(dataloader) + i\n",
    "        all_loss_G = avg_loss_G * curr_iter\n",
    "        all_loss_D = avg_loss_D * curr_iter\n",
    "        all_loss_A = avg_loss_A * curr_iter\n",
    "        all_loss_G += errG.data.item()\n",
    "        all_loss_D += errD.data.item()\n",
    "        all_loss_A += accuracy\n",
    "        avg_loss_G = all_loss_G / (curr_iter + 1)\n",
    "        avg_loss_D = all_loss_D / (curr_iter + 1)\n",
    "        avg_loss_A = all_loss_A / (curr_iter + 1)\n",
    "\n",
    "    print('[%d/%d][%d/%d] Loss_D: %.4f (%.4f) Loss_G: %.4f (%.4f) D(x): %.4f D(G(z)): %.4f / %.4f Acc: %.4f (%.4f)'\n",
    "          % (epoch, epochs, i, len(dataloader),\n",
    "             errD.data.item(), avg_loss_D, errG.data.item(), avg_loss_G, D_x, D_G_z1, D_G_z2, accuracy, avg_loss_A))\n",
    "    if epoch % 5 == 0:\n",
    "        vutils.save_image(\n",
    "            real_cpu, '%s/real_samples.png' % 'premade')\n",
    "        print('Label for eval = {}'.format(eval_label))\n",
    "        fake = netG(eval_noise)\n",
    "        vutils.save_image(\n",
    "            fake.data,\n",
    "            '%s/fake_samples_epoch_%03d.png' % ('premade', epoch)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(netG.state_dict(), '%s/netG_epoch_100.pth' % ('premade'))\n",
    "torch.save(netD.state_dict(), '%s/netD_epoch_100.pth' % ('premade'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
